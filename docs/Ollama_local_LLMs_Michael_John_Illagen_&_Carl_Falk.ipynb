{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3403182-e5f5-4da9-bc10-033d2bfc5e91",
   "metadata": {},
   "source": [
    "# Generating items via local LLM (Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a2305-a45d-44c2-a21f-823b4f61d593",
   "metadata": {},
   "source": [
    "In the previous section, we demonstrated AI item generation using an LLM via API access to a cloud-based system. Under the hood, this required sending the prompts to OpenAI servers as well as receiving a reply from them. At the time of writing this, there is more than one way to run an LLM locally (e.g., on your or your organization's computer), instead of using a model in the cloud. Major options include Huggingface's `transformers` module, or Ollama.\n",
    "\n",
    "There are generally security, speed/performance, and cost tradeoffs involved in deciding to run a model locally. Processing everything locally means that there is no need to pay an AI provider (e.g., OpenAI) and no potentially sensitive data is transmitted via the internet. On the other hand, there are overhead costs: One may need to invest in a powerful local machine and GPU in order to be able to run large models locally and obtain results in a reasonable amount of time.\n",
    "\n",
    "In this chapter, we generate items using Ollama and an open-source model (e.g. gpt-oss) of your choice. The present chapter closely follows the chapter \"Generating items via an API\", but note that there are different steps, as some steps (e.g. getting an API key) are not applicable to working locally. Some small prompt changes were also implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6542af-e000-40e9-8ae6-6901f1ff8aac",
   "metadata": {},
   "source": [
    "## Step 1. Install Ollama and start it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248379c-d137-4638-be6e-804cfd01eebd",
   "metadata": {},
   "source": [
    "Models from Ollama exist in an environment independent from Python yet can be accessed (entirely locally) via an API.\n",
    "One can even start a graphical user interface to chat with Ollama models, entirely locally.\n",
    "It is currently available for major operating systems (Windows, macOS, Linux).\n",
    "To get started, download and install Ollama from here: https://ollama.com/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5e5f2-5901-433c-bdd1-c4ae2a79e97a",
   "metadata": {},
   "source": [
    "Once Ollama is downloaded and installed, you have to start the service. Executing the following command in a terminal or powershell window  allows you to send requests to it in later steps. Note that the ampersand is important, as it indicates that Ollama is to be run as a background process. This can't be run in a Jupyter cell because Jupyter does not support background processes. Subprocess.Popen() is discouraged because the process can't be killed from within the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30353225-4e72-43ae-9082-97da14039b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as shell command to start Ollama\n",
    "# ollama serve &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527c578-9642-4ce2-bd7b-87aa8fe84f4c",
   "metadata": {},
   "source": [
    "## Step 2. Download the local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694f6d1-df49-43b3-a4e4-8f6142fa2dc9",
   "metadata": {},
   "source": [
    "To download a local LLM for use with Ollama (only required once), execute the following command in a terminal window or from within Jupyter by prefixing with ! (in Jupyter is fine here, as the command does not involve a background process). In this example, we use the model `gpt-oss:20b` (i.e. Open AI's open weight reasoning model with 20 billion parameters), which you can substitute with the model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe2286e-ef76-49cc-910c-08634e871ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as shell command to download an LLM, first time only\n",
    "# !ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bacb3d-962d-44d7-803f-b9ac70e7df53",
   "metadata": {},
   "source": [
    "To see the possible models you can use, go to Ollama's [model catalog](https://ollama.com/models).\n",
    "\n",
    "Of course, it is possible to have multiple models downloaded.\n",
    "When you send a request to Ollama in a later step, you will specify which model to use.\n",
    "To see a list of models you have downloaded, execute the following command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eacc7442-f684-46e0-9836-a8cc2cf2b235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ollama server not responding - could not find ollama app\n"
     ]
    }
   ],
   "source": [
    "# Run as shell command to see list of downloaded LLMs\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0ccf6-91b5-4a13-a1c8-2cc261a75de0",
   "metadata": {},
   "source": [
    "## Step 3. Install the Ollama Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3b6eb-06d3-428b-a207-0e81b3b90a1a",
   "metadata": {},
   "source": [
    "To install the Ollama Python library (only required once), execute the following command in a terminal window. \n",
    "Doing so lets us send requests to Ollama from our Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d6f361-66af-4b78-9e8a-3eb5f931d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as shell command to install Ollama Python library\n",
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27012cff-03e4-4b2a-ade2-70aaf9679d63",
   "metadata": {},
   "source": [
    "## Step 4. Open a Jupyter notebook\n",
    "\n",
    "If desired to execute Python comments in a Jupyter Notebook, navigate in the terminal to the folder where you are storing your work and run the command `jupyter notebook`. This will launch a new Jupyter notebook Interface on the local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "826aa787-9546-4a66-a64b-f786698a58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as shell command to start Jupyter\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5cf57-a7fd-4fe4-9477-be60a6678255",
   "metadata": {},
   "source": [
    "Now once we select ‘new notebook’ we are ready to code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafe2f0-fe34-4793-a879-1154fced7435",
   "metadata": {},
   "source": [
    "## Step 5. Generate items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d83db8-812c-438b-9945-fe99931a67bb",
   "metadata": {},
   "source": [
    "Requests to Ollama have multiple parameters or arguments.\n",
    "Most important is the model handle, which is the name of the model you previously downloaded and now want to use.\n",
    "In addition, there are options, such and the maximum number of tokens generated (called `num_predict` in Ollama), the reproducibility seed (called `seed` in Ollama), and ensure model requests do not time out by picking a large number for `keep_alive` (here, 1 hour).\n",
    "In Python, we store this information now to be used in later code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38fc4bee-232b-4706-8003-107e16060bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama arguments\n",
    "llm_handle = 'gpt-oss:20b'\n",
    "ollama_options = {'seed' : 781, 'num_predict' : 3000, 'keep_alive' : \"1h\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b115132-fda7-476c-a190-3411c65b9159",
   "metadata": {},
   "source": [
    "Now we’re ready to generate items. In this example, we will make 12 API calls to systematically generate items for the moral foundations. \n",
    "In this example, we generate generate 25 positively keyed items and 25 reverse scored items for each moral foundation.\n",
    "Although one may attempt to generate a larger number of items as in the \"Generating items via an API\" chapter, testing and debugging local LLMs may be easier with smaller batches of items.\n",
    "Without more powerfull hardware (use of GPUs is not yet demonstrated) or a much smaller parameter model, responses from the LLM will be slower.\n",
    "In the example here, we again demonstrate guided item generation without sample items but sample items can easily be incorporated when they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee259a3-e304-4729-b34c-512327667e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import ollama\n",
    "\n",
    "# Moral foundation definitions\n",
    "\n",
    "foundation_definitions = {\n",
    "    \"Care/Harm\": \"\"\"Caring and compassion toward others' suffering versus indifference or acceptance of harm if it is in pursuit of business goals.\"\"\",\n",
    "    \"Fairness/Cheating\": \"\"\"Commitment to fairness and proportionality in dealings with others; contrasted with comfort with exploitation or uneven application of standards.\"\"\",\n",
    "    \"Loyalty/Betrayal\": \"\"\"Loyalty, patriotism, and self-sacrifice toward one's group; contrasted with indifference to group ties or little concern with disloyalty.\"\"\",\n",
    "    \"Authority/Subversion\": \"\"\"Obedience and deference toward legitimate authority and traditions; contrasted with skepticism of authority or comfort with challenging established norms.\"\"\",\n",
    "    \"Sanctity/Degradation\": \"\"\"Respect for what is considered pure or noble; contrasted with indifference or disregard toward those standards.\"\"\",\n",
    "    \"Liberty/Oppression\": \"\"\"Respect for autonomy and aversion toward excessive control; contrasted with willingness to dominate others and restrict their legitimate freedoms.\"\"\"\n",
    "}\n",
    "\n",
    "foundations = [\n",
    "    \"Care/Harm\", \"Fairness/Cheating\", \"Loyalty/Betrayal\",\n",
    "    \"Authority/Subversion\", \"Sanctity/Degradation\", \"Liberty/Oppression\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5def6b31-e91e-4478-8ec3-057b52778c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Pos items for Care/Harm...\n",
      "✓ Done: 25 items\n",
      "Generating Rev items for Care/Harm...\n",
      "✓ Done: 26 items\n",
      "Generating Pos items for Fairness/Cheating...\n",
      "✓ Done: 25 items\n",
      "Generating Rev items for Fairness/Cheating...\n",
      "✓ Done: 25 items\n",
      "Generating Pos items for Loyalty/Betrayal...\n",
      "✓ Done: 25 items\n",
      "Generating Rev items for Loyalty/Betrayal...\n",
      "✓ Done: 25 items\n",
      "Generating Pos items for Authority/Subversion...\n",
      "✓ Done: 25 items\n",
      "Generating Rev items for Authority/Subversion...\n",
      "✓ Done: 25 items\n",
      "Generating Pos items for Sanctity/Degradation...\n",
      "✓ Done: 25 items\n",
      "Generating Rev items for Sanctity/Degradation...\n",
      "✓ Done: 25 items\n",
      "Generating Pos items for Liberty/Oppression...\n",
      "✓ Done: 25 items\n",
      "Generating Rev items for Liberty/Oppression...\n",
      "✓ Done: 25 items\n"
     ]
    }
   ],
   "source": [
    "with open(\"moral_foundations_items.csv\", \"w\", newline=\"\", encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Foundation\", \"Item\", \"Keyed\"])\n",
    "    \n",
    "    for f in foundations:\n",
    "        for k, label in [(\"positively keyed\", \"Pos\"), (\"negatively keyed/reverse scored\", \"Rev\")]:\n",
    "            print(f\"Generating {label} items for {f}...\")\n",
    "            prompt = f\"\"\"\n",
    "You are a psychological assessment expert. You write work-related, clear, Likert-style items for measuring moral values in executive leaders.\n",
    "Generate 25 {k} items for the {f} moral foundation.\n",
    "Foundation definition:\n",
    "{foundation_definitions[f]}\n",
    "Each item must:\n",
    "- Do not use the scale name in the item\n",
    "- Reflect **attitudes**, not behaviors\n",
    "- Be relevant to **leadership or workplace contexts**\n",
    "- Be **no more than 10 words long**\n",
    "- Make **reversed items subtle so they are not socially undesirable**\n",
    "- Avoid **double-barreled statements**\n",
    "- Be written at a **simple reading level** (grade 6–8)\n",
    "- Use **plain, concise language**\n",
    "- Avoid **repetitive item phrasing**\n",
    "Likert scale:\n",
    "1 = Strongly Disagree ... 5 = Strongly Agree\n",
    "\n",
    "Place each item on a new line and do not provide any extra explanation; no need to say 'here are the 25 items'.\n",
    "Just list the items and no more.\n",
    "\"\"\"\n",
    "            try:\n",
    "                res = ollama.generate(\n",
    "                    model=llm_handle,\n",
    "                    prompt=prompt,\n",
    "                    options=ollama_options\n",
    "                )\n",
    "                items = res['response'].strip().split(\"\\n\")\n",
    "                for line in items:\n",
    "                    writer.writerow([f, line.strip(), label])\n",
    "                print(f\"✓ Done: {len(items)} items\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed for {f} ({label}):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b415bc5-fad5-4576-8968-d4801def5be9",
   "metadata": {},
   "source": [
    "## Steps 6 & 7. Item post processing and human item reviews\n",
    "\n",
    "From this point forward, Ollama is no longer used. These remaining steps are the same as in the chapter \"Generating items via an API\".\n",
    "As a re-cap, the generative A.I. model might not always perform exactly as expected.\n",
    "It may generate a slightly different number of items than expected and the formatting may not be quite ready for pilot/field testing with humans due to additional numbering or added text explanation.\n",
    "Some of these issues can sometimes be addressed with additional prompt engineering, which we have attempted to do so in this example.\n",
    "\n",
    "In addition, not all items may follow exactly what is expected of the prompt.\n",
    "In some test cases with models with fewer parameters, we noticed that the model had more frequent problems with understanding positively/negatively keyed items and getting this wording correct.\n",
    "If more items are desired, consider changing the random number seed and the output file name, then re-running the code.\n",
    "\n",
    "In all cases, we strongly recommend human reviews of the content of all items proposed to go to empirical trials and also stress the need for these empirical evaluations to get indications of actual item discrimination. Human reviews will allow evaluation of when items are too similar and It is always empirical discrimination that is the ultimate parameter of interest in item analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31e583-4603-4a11-8fa1-d93ecf1fdf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
