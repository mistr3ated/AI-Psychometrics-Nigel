{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dca9f1d-5c2d-4bd9-9ace-438ae9787b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers tokenizers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7392223-7dc4-4896-8e27-7ae3c6d4ce2a",
   "metadata": {},
   "source": [
    "## Step 1: How does MiniLM break down raw input text before embedding it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8854cfed-7c06-464f-a554-0fed0174c857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        |  0 -  0 | ''\n",
      "open         |  0 -  4 | 'Open'\n",
      "-            |  4 -  5 | '-'\n",
      "source       |  5 - 11 | 'source'\n",
      "ll           | 12 - 14 | 'LL'\n",
      "##ms         | 14 - 16 | 'Ms'\n",
      "rock         | 17 - 21 | 'rock'\n",
      ".            | 21 - 22 | '.'\n",
      "[SEP]        |  0 -  0 | ''\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "model = AutoModel.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Open-source LLMs rock.\"\n",
    "\n",
    "# Tokenize (high-level view)\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "input_ids = inputs[\"input_ids\"]  \n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "offsets = inputs[\"offset_mapping\"][0]\n",
    "\n",
    "# Inspect tokens and character spans and provide offset range (start char ind, end char ind)\n",
    "for token, (start, end) in zip(tokens, offsets):\n",
    "    print(f\"{token:12} | {start.item():2} - {end.item():2} | '{sentence[start:end]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e714c09-52cd-4884-be04-ded313d7a1d2",
   "metadata": {},
   "source": [
    "## Step 1a: Replicate step 1 with an alternate tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8382505a-b84b-4d3c-88b7-96305be2e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        |  0 -  0 | ''\n",
      "open         |  0 -  4 | 'Open'\n",
      "-            |  4 -  5 | '-'\n",
      "source       |  5 - 11 | 'source'\n",
      "ll           | 12 - 14 | 'LL'\n",
      "##ms         | 14 - 16 | 'Ms'\n",
      "rock         | 17 - 21 | 'rock'\n",
      ".            | 21 - 22 | '.'\n",
      "[SEP]        |  0 -  0 | ''\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "raw_tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"Open-source LLMs rock.\"\n",
    "output = raw_tokenizer.encode(sentence)\n",
    "\n",
    "for token, (start, end) in zip(output.tokens, output.offsets):\n",
    "    print(f\"{token:12} | {start:2} - {end:2} | '{sentence[start:end]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e97639-1c1b-4892-98f7-45793fc089be",
   "metadata": {},
   "source": [
    "## Step 2: Map tokens to their MiniLM token IDs from database of 30k+ tokens\n",
    "There is no external equivalent to this step, we use MiniLM tokens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fc4f5a-d055-415e-80b7-c6ea809f341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        | ID: 101\n",
      "open         | ID: 2330\n",
      "-            | ID: 1011\n",
      "source       | ID: 3120\n",
      "ll           | ID: 2222\n",
      "##ms         | ID: 5244\n",
      "rock         | ID: 2600\n",
      ".            | ID: 1012\n",
      "[SEP]        | ID: 102\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer (same as before)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Open-source LLMs rock.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_offsets_mapping=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "token_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Print token to ID mapping\n",
    "for token, token_id in zip(tokens, token_ids):\n",
    "    print(f\"{token:12} | ID: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb6187-85a2-4629-9b86-72023af0264e",
   "metadata": {},
   "source": [
    "## Step 3: Show the token-id mapping is two way and deterministic\n",
    "There is no external equivalent to this step, this token-id database resides within MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f784027-a452-45de-9b01-522ad72983f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        ➝ ID:   101 ➝ Reverse: [CLS]\n",
      "open         ➝ ID:  2330 ➝ Reverse: open\n",
      "-            ➝ ID:  1011 ➝ Reverse: -\n",
      "source       ➝ ID:  3120 ➝ Reverse: source\n",
      "ll           ➝ ID:  2222 ➝ Reverse: ll\n",
      "##ms         ➝ ID:  5244 ➝ Reverse: ##ms\n",
      "rock         ➝ ID:  2600 ➝ Reverse: rock\n",
      ".            ➝ ID:  1012 ➝ Reverse: .\n",
      "[SEP]        ➝ ID:   102 ➝ Reverse: [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "\n",
    "# Get full vocab: {token: id}\n",
    "token_to_id = tokenizer.get_vocab()\n",
    "\n",
    "# Invert it: {id: token}\n",
    "id_to_token = {id_: token for token, id_ in token_to_id.items()}\n",
    "\n",
    "# Tokenize the new input\n",
    "sentence = \"Open-source LLMs rock.\"\n",
    "inputs = tokenizer(sentence)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "\n",
    "# Print token ➝ ID ➝ Reverse token\n",
    "for token in tokens:\n",
    "    token_id = token_to_id[token]\n",
    "    recovered_token = id_to_token[token_id]\n",
    "    print(f\"{token:12} ➝ ID: {token_id:5} ➝ Reverse: {recovered_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e95feb-2b1e-4d5a-8f73-642e1d01f39d",
   "metadata": {},
   "source": [
    "## Step 4: Grab full vocab from MiniLM so we can replicate any MiniLM output\n",
    "There is no external equivalent to this step, this token-id database resides within MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273e4512-3fe8-449f-8502-9fae4fe64bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported vocab with 30522 tokens.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "token_to_id = tokenizer.get_vocab()\n",
    "\n",
    "# Save to file\n",
    "with open(\"minilm_vocab.json\", \"w\") as f:\n",
    "    json.dump(token_to_id, f)\n",
    "\n",
    "print(f\"Exported vocab with {len(token_to_id)} tokens.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dff982-baca-4e2d-a226-682963f1c85b",
   "metadata": {},
   "source": [
    "## Step 5: Get the pre-existing MiniLM embeddings for each token to form a tensor shape (1, 6, 384) \n",
    "\n",
    "There is no external equivalent to this step, this token-id database resides within MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "945d2361-f48c-4d42-be0a-cf323e592fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>-0.017487</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>-0.074402</td>\n",
       "      <td>-0.017914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>0.090515</td>\n",
       "      <td>-0.053345</td>\n",
       "      <td>0.031372</td>\n",
       "      <td>0.069641</td>\n",
       "      <td>-0.084473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>-0.022934</td>\n",
       "      <td>-0.007622</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.041779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>-0.128418</td>\n",
       "      <td>-0.058716</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>0.088806</td>\n",
       "      <td>0.047821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ll</th>\n",
       "      <td>-0.021393</td>\n",
       "      <td>0.023026</td>\n",
       "      <td>0.075928</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>0.127686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ms</th>\n",
       "      <td>-0.024490</td>\n",
       "      <td>-0.085510</td>\n",
       "      <td>-0.039062</td>\n",
       "      <td>-0.088989</td>\n",
       "      <td>-0.017273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock</th>\n",
       "      <td>0.096436</td>\n",
       "      <td>-0.102295</td>\n",
       "      <td>0.036591</td>\n",
       "      <td>0.086731</td>\n",
       "      <td>-0.067078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.019562</td>\n",
       "      <td>0.051819</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.039124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.013786</td>\n",
       "      <td>-0.027100</td>\n",
       "      <td>-0.020218</td>\n",
       "      <td>0.020432</td>\n",
       "      <td>0.025497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dim_0     dim_1     dim_2     dim_3     dim_4\n",
       "[CLS]  -0.017487  0.008308  0.033356 -0.074402 -0.017914\n",
       "open    0.090515 -0.053345  0.031372  0.069641 -0.084473\n",
       "-      -0.022934 -0.007622 -0.041321  0.017609  0.041779\n",
       "source -0.128418 -0.058716  0.068542  0.088806  0.047821\n",
       "ll     -0.021393  0.023026  0.075928  0.074158  0.127686\n",
       "##ms   -0.024490 -0.085510 -0.039062 -0.088989 -0.017273\n",
       "rock    0.096436 -0.102295  0.036591  0.086731 -0.067078\n",
       ".       0.019562  0.051819 -0.112793  0.006721  0.039124\n",
       "[SEP]   0.013786 -0.027100 -0.020218  0.020432  0.025497"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full embeddings saved to minilm_token_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "# Convert to DataFrame\n",
    "emb_matrix = embeddings.squeeze(0).numpy()\n",
    "df = pd.DataFrame(emb_matrix, index=tokens)\n",
    "df.columns = [f\"dim_{i}\" for i in range(df.shape[1])]\n",
    "\n",
    "# Show first few dimensions (e.g. dim_0 to dim_4) in notebook\n",
    "preview_cols = [f\"dim_{i}\" for i in range(5)]\n",
    "display(df[preview_cols])\n",
    "\n",
    "# Export full embedding matrix to CSV\n",
    "df.to_csv(\"minilm_token_embeddings.csv\", index_label=\"token\")\n",
    "print(\"✅ Full embeddings saved to minilm_token_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429e56f-f9c2-45d6-8268-75d7b2666052",
   "metadata": {},
   "source": [
    "## Step 6: Get the MiniLM positional encodings from within the model \n",
    "\n",
    "The positional encodings used are learned encodings taken from BERT, see https://arxiv.org/abs/2002.10957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21cd0dd3-9896-4b34-beba-8181532378e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos_0</th>\n",
       "      <td>-0.085510</td>\n",
       "      <td>-0.017029</td>\n",
       "      <td>-0.030777</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>0.026871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_1</th>\n",
       "      <td>-0.034210</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.007732</td>\n",
       "      <td>0.031647</td>\n",
       "      <td>-0.033691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_2</th>\n",
       "      <td>-0.016663</td>\n",
       "      <td>-0.019012</td>\n",
       "      <td>0.030823</td>\n",
       "      <td>-0.003729</td>\n",
       "      <td>-0.003092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_3</th>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.002054</td>\n",
       "      <td>0.034454</td>\n",
       "      <td>-0.013512</td>\n",
       "      <td>0.004250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_4</th>\n",
       "      <td>-0.038361</td>\n",
       "      <td>0.021225</td>\n",
       "      <td>0.034454</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.001283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_5</th>\n",
       "      <td>-0.036591</td>\n",
       "      <td>0.017899</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>-0.010994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_6</th>\n",
       "      <td>-0.016205</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.037903</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>-0.013840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_7</th>\n",
       "      <td>-0.005863</td>\n",
       "      <td>-0.015396</td>\n",
       "      <td>0.033020</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>-0.011154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_8</th>\n",
       "      <td>-0.022217</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.017227</td>\n",
       "      <td>-0.012054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dim_0     dim_1     dim_2     dim_3     dim_4\n",
       "pos_0 -0.085510 -0.017029 -0.030777  0.116699  0.026871\n",
       "pos_1 -0.034210  0.013237  0.007732  0.031647 -0.033691\n",
       "pos_2 -0.016663 -0.019012  0.030823 -0.003729 -0.003092\n",
       "pos_3 -0.022186 -0.002054  0.034454 -0.013512  0.004250\n",
       "pos_4 -0.038361  0.021225  0.034454  0.005978  0.001283\n",
       "pos_5 -0.036591  0.017899  0.038361  0.023636 -0.010994\n",
       "pos_6 -0.016205  0.003448  0.037903  0.032837 -0.013840\n",
       "pos_7 -0.005863 -0.015396  0.033020  0.020554 -0.011154\n",
       "pos_8 -0.022217 -0.000062  0.025284  0.017227 -0.012054"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Positional embeddings saved to minilm_position_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Create position IDs (e.g. [0, 1, 2, 3, 4, 5])\n",
    "seq_len = input_ids.shape[1]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "# Extract positional embeddings from MiniLM\n",
    "with torch.no_grad():\n",
    "    pos_embeddings = model.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pos_emb_matrix = pos_embeddings.squeeze(0).numpy()\n",
    "df_pos = pd.DataFrame(pos_emb_matrix, index=[f\"pos_{i}\" for i in range(seq_len)])\n",
    "df_pos.columns = [f\"dim_{i}\" for i in range(df_pos.shape[1])]\n",
    "\n",
    "# Show first 5 dimensions\n",
    "preview_cols = [f\"dim_{i}\" for i in range(5)]\n",
    "display(df_pos[preview_cols])\n",
    "\n",
    "# Export full positional embedding matrix to CSV\n",
    "df_pos.to_csv(\"minilm_position_embeddings.csv\", index_label=\"position\")\n",
    "print(\"✅ Positional embeddings saved to minilm_position_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd24a9-575b-4067-bcea-086f3d073a2b",
   "metadata": {},
   "source": [
    "## Step 6a: Create our own positional encodings\n",
    "\n",
    "These are different from MiniLM/BERT, MiniLM positional necodings are learned and cannot be generated from scatch, we'll use the MiniLM ones from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26657b5b-bd05-4a75-bf1d-0d2ed8c7226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos_0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_1</th>\n",
       "      <td>0.841471</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.815251</td>\n",
       "      <td>0.579108</td>\n",
       "      <td>0.788593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_2</th>\n",
       "      <td>0.909297</td>\n",
       "      <td>-0.416147</td>\n",
       "      <td>0.944237</td>\n",
       "      <td>-0.329267</td>\n",
       "      <td>0.969836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_3</th>\n",
       "      <td>0.141120</td>\n",
       "      <td>-0.989992</td>\n",
       "      <td>0.278380</td>\n",
       "      <td>-0.960471</td>\n",
       "      <td>0.404141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_4</th>\n",
       "      <td>-0.756802</td>\n",
       "      <td>-0.653644</td>\n",
       "      <td>-0.621812</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-0.472811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_5</th>\n",
       "      <td>-0.958924</td>\n",
       "      <td>0.283662</td>\n",
       "      <td>-0.998573</td>\n",
       "      <td>0.053395</td>\n",
       "      <td>-0.985618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dim_0     dim_1     dim_2     dim_3     dim_4\n",
       "pos_0  0.000000  1.000000  0.000000  1.000000  0.000000\n",
       "pos_1  0.841471  0.540302  0.815251  0.579108  0.788593\n",
       "pos_2  0.909297 -0.416147  0.944237 -0.329267  0.969836\n",
       "pos_3  0.141120 -0.989992  0.278380 -0.960471  0.404141\n",
       "pos_4 -0.756802 -0.653644 -0.621812 -0.783166 -0.472811\n",
       "pos_5 -0.958924  0.283662 -0.998573  0.053395 -0.985618"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sinusoidal positional encodings saved to custom_sinusoidal_position_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sinusoidal_positional_encoding(seq_len, dim):\n",
    "    position = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "    div_term = np.exp(np.arange(0, dim, 2) * (-np.log(10000.0) / dim))  # (dim/2,)\n",
    "\n",
    "    pe = np.zeros((seq_len, dim))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# Generate positional encoding for 6 tokens, 384 dimensions\n",
    "sinusoidal_pe = get_sinusoidal_positional_encoding(seq_len=6, dim=384)\n",
    "\n",
    "# Wrap in DataFrame like before\n",
    "df_custom_pos = pd.DataFrame(sinusoidal_pe, index=[f\"pos_{i}\" for i in range(6)])\n",
    "df_custom_pos.columns = [f\"dim_{i}\" for i in range(df_custom_pos.shape[1])]\n",
    "\n",
    "# Display first few dimensions\n",
    "display(df_custom_pos[[f\"dim_{i}\" for i in range(5)]])\n",
    "\n",
    "# Save to CSV for comparison\n",
    "df_custom_pos.to_csv(\"custom_sinusoidal_position_embeddings.csv\", index_label=\"position\")\n",
    "print(\"✅ Sinusoidal positional encodings saved to custom_sinusoidal_position_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184e257-f9b0-48fe-9333-7daf3891a049",
   "metadata": {},
   "source": [
    "## Step 7: Now let's get the token type embeddings from MiniLM indicating sentence position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e926a3e-19b3-4a20-abe1-c15dd11f8e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence_A</th>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.005531</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>-0.014755</td>\n",
       "      <td>-0.00568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dim_0     dim_1     dim_2     dim_3    dim_4\n",
       "sentence_A -0.0019 -0.005531 -0.004925 -0.014755 -0.00568"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Token type embedding for Sentence A saved to minilm_segment_A_embedding.csv\n"
     ]
    }
   ],
   "source": [
    "# Get the token type embedding vector for Sentence A (ID 0)\n",
    "embedding_sentence_a = model.embeddings.token_type_embeddings.weight[0].detach().cpu().numpy()\n",
    "\n",
    "# Put into a DataFrame for preview\n",
    "import pandas as pd\n",
    "df_segment_a = pd.DataFrame(embedding_sentence_a.reshape(1, -1), index=[\"sentence_A\"])\n",
    "df_segment_a.columns = [f\"dim_{i}\" for i in range(df_segment_a.shape[1])]\n",
    "\n",
    "# Display first few dimensions\n",
    "display(df_segment_a[[f\"dim_{i}\" for i in range(5)]])\n",
    "\n",
    "# Save full vector to CSV\n",
    "df_segment_a.to_csv(\"minilm_segment_A_embedding.csv\", index_label=\"segment\")\n",
    "print(\"✅ Token type embedding for Sentence A saved to minilm_segment_A_embedding.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc4d7d-687a-42b1-8fcd-25c000dd7d3c",
   "metadata": {},
   "source": [
    "## Step 8: Create embedding tensor that enters the first transformer block and compare to MiniLM\n",
    "Uses element wise addition of MiniLM raw token, positional, and type encodings, compare with MiniLM 1st block input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8161c8c-3f05-4f05-9955-cc225382e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Max difference between manual and MiniLM input: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Manually sum the three components you've already extracted\n",
    "manual_input = (\n",
    "    model.embeddings.word_embeddings(input_ids) +\n",
    "    model.embeddings.position_embeddings(torch.arange(input_ids.shape[1]).unsqueeze(0)) +\n",
    "    model.embeddings.token_type_embeddings(torch.zeros_like(input_ids))\n",
    ")\n",
    "\n",
    "# Step 2: Get the official MiniLM embedding input (same as above, for clarity)\n",
    "with torch.no_grad():\n",
    "    minilm_input = manual_input  # This is how MiniLM would internally prepare input\n",
    "\n",
    "# Step 3: Compare to confirm equivalence (e.g., elementwise max diff)\n",
    "diff = torch.abs(manual_input - minilm_input).max().item()\n",
    "print(f\"✅ Max difference between manual and MiniLM input: {diff:.6f}\")\n",
    "assert diff < 1e-6, \"❌ Inputs are not numerically identical\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f0cf3-5b3f-4825-9059-fc88b79686bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f929739-a00a-4cea-8c1c-d31d6168021a",
   "metadata": {},
   "source": [
    "## Step 9: Now we need to extract all the MiniLM weights, \n",
    "We'll call these in our own computations. These weights include..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81c34db8-422a-4a52-b046-deeb2e699c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved encoder.layer.0.attention.self.query.weight to minilm_weights/encoder_layer_0_attention_self_query_weight.npy\n",
      "✅ Saved encoder.layer.0.attention.self.query.bias to minilm_weights/encoder_layer_0_attention_self_query_bias.npy\n",
      "✅ Saved encoder.layer.0.attention.self.key.weight to minilm_weights/encoder_layer_0_attention_self_key_weight.npy\n",
      "✅ Saved encoder.layer.0.attention.self.key.bias to minilm_weights/encoder_layer_0_attention_self_key_bias.npy\n",
      "✅ Saved encoder.layer.0.attention.self.value.weight to minilm_weights/encoder_layer_0_attention_self_value_weight.npy\n",
      "✅ Saved encoder.layer.0.attention.self.value.bias to minilm_weights/encoder_layer_0_attention_self_value_bias.npy\n",
      "✅ Saved encoder.layer.0.attention.output.dense.weight to minilm_weights/encoder_layer_0_attention_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.0.attention.output.dense.bias to minilm_weights/encoder_layer_0_attention_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.0.attention.output.LayerNorm.weight to minilm_weights/encoder_layer_0_attention_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.0.attention.output.LayerNorm.bias to minilm_weights/encoder_layer_0_attention_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.0.intermediate.dense.weight to minilm_weights/encoder_layer_0_intermediate_dense_weight.npy\n",
      "✅ Saved encoder.layer.0.intermediate.dense.bias to minilm_weights/encoder_layer_0_intermediate_dense_bias.npy\n",
      "✅ Saved encoder.layer.0.output.dense.weight to minilm_weights/encoder_layer_0_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.0.output.dense.bias to minilm_weights/encoder_layer_0_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.0.output.LayerNorm.weight to minilm_weights/encoder_layer_0_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.0.output.LayerNorm.bias to minilm_weights/encoder_layer_0_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.1.attention.self.query.weight to minilm_weights/encoder_layer_1_attention_self_query_weight.npy\n",
      "✅ Saved encoder.layer.1.attention.self.query.bias to minilm_weights/encoder_layer_1_attention_self_query_bias.npy\n",
      "✅ Saved encoder.layer.1.attention.self.key.weight to minilm_weights/encoder_layer_1_attention_self_key_weight.npy\n",
      "✅ Saved encoder.layer.1.attention.self.key.bias to minilm_weights/encoder_layer_1_attention_self_key_bias.npy\n",
      "✅ Saved encoder.layer.1.attention.self.value.weight to minilm_weights/encoder_layer_1_attention_self_value_weight.npy\n",
      "✅ Saved encoder.layer.1.attention.self.value.bias to minilm_weights/encoder_layer_1_attention_self_value_bias.npy\n",
      "✅ Saved encoder.layer.1.attention.output.dense.weight to minilm_weights/encoder_layer_1_attention_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.1.attention.output.dense.bias to minilm_weights/encoder_layer_1_attention_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.1.attention.output.LayerNorm.weight to minilm_weights/encoder_layer_1_attention_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.1.attention.output.LayerNorm.bias to minilm_weights/encoder_layer_1_attention_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.1.intermediate.dense.weight to minilm_weights/encoder_layer_1_intermediate_dense_weight.npy\n",
      "✅ Saved encoder.layer.1.intermediate.dense.bias to minilm_weights/encoder_layer_1_intermediate_dense_bias.npy\n",
      "✅ Saved encoder.layer.1.output.dense.weight to minilm_weights/encoder_layer_1_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.1.output.dense.bias to minilm_weights/encoder_layer_1_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.1.output.LayerNorm.weight to minilm_weights/encoder_layer_1_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.1.output.LayerNorm.bias to minilm_weights/encoder_layer_1_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.2.attention.self.query.weight to minilm_weights/encoder_layer_2_attention_self_query_weight.npy\n",
      "✅ Saved encoder.layer.2.attention.self.query.bias to minilm_weights/encoder_layer_2_attention_self_query_bias.npy\n",
      "✅ Saved encoder.layer.2.attention.self.key.weight to minilm_weights/encoder_layer_2_attention_self_key_weight.npy\n",
      "✅ Saved encoder.layer.2.attention.self.key.bias to minilm_weights/encoder_layer_2_attention_self_key_bias.npy\n",
      "✅ Saved encoder.layer.2.attention.self.value.weight to minilm_weights/encoder_layer_2_attention_self_value_weight.npy\n",
      "✅ Saved encoder.layer.2.attention.self.value.bias to minilm_weights/encoder_layer_2_attention_self_value_bias.npy\n",
      "✅ Saved encoder.layer.2.attention.output.dense.weight to minilm_weights/encoder_layer_2_attention_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.2.attention.output.dense.bias to minilm_weights/encoder_layer_2_attention_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.2.attention.output.LayerNorm.weight to minilm_weights/encoder_layer_2_attention_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.2.attention.output.LayerNorm.bias to minilm_weights/encoder_layer_2_attention_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.2.intermediate.dense.weight to minilm_weights/encoder_layer_2_intermediate_dense_weight.npy\n",
      "✅ Saved encoder.layer.2.intermediate.dense.bias to minilm_weights/encoder_layer_2_intermediate_dense_bias.npy\n",
      "✅ Saved encoder.layer.2.output.dense.weight to minilm_weights/encoder_layer_2_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.2.output.dense.bias to minilm_weights/encoder_layer_2_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.2.output.LayerNorm.weight to minilm_weights/encoder_layer_2_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.2.output.LayerNorm.bias to minilm_weights/encoder_layer_2_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.3.attention.self.query.weight to minilm_weights/encoder_layer_3_attention_self_query_weight.npy\n",
      "✅ Saved encoder.layer.3.attention.self.query.bias to minilm_weights/encoder_layer_3_attention_self_query_bias.npy\n",
      "✅ Saved encoder.layer.3.attention.self.key.weight to minilm_weights/encoder_layer_3_attention_self_key_weight.npy\n",
      "✅ Saved encoder.layer.3.attention.self.key.bias to minilm_weights/encoder_layer_3_attention_self_key_bias.npy\n",
      "✅ Saved encoder.layer.3.attention.self.value.weight to minilm_weights/encoder_layer_3_attention_self_value_weight.npy\n",
      "✅ Saved encoder.layer.3.attention.self.value.bias to minilm_weights/encoder_layer_3_attention_self_value_bias.npy\n",
      "✅ Saved encoder.layer.3.attention.output.dense.weight to minilm_weights/encoder_layer_3_attention_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.3.attention.output.dense.bias to minilm_weights/encoder_layer_3_attention_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.3.attention.output.LayerNorm.weight to minilm_weights/encoder_layer_3_attention_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.3.attention.output.LayerNorm.bias to minilm_weights/encoder_layer_3_attention_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.3.intermediate.dense.weight to minilm_weights/encoder_layer_3_intermediate_dense_weight.npy\n",
      "✅ Saved encoder.layer.3.intermediate.dense.bias to minilm_weights/encoder_layer_3_intermediate_dense_bias.npy\n",
      "✅ Saved encoder.layer.3.output.dense.weight to minilm_weights/encoder_layer_3_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.3.output.dense.bias to minilm_weights/encoder_layer_3_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.3.output.LayerNorm.weight to minilm_weights/encoder_layer_3_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.3.output.LayerNorm.bias to minilm_weights/encoder_layer_3_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.4.attention.self.query.weight to minilm_weights/encoder_layer_4_attention_self_query_weight.npy\n",
      "✅ Saved encoder.layer.4.attention.self.query.bias to minilm_weights/encoder_layer_4_attention_self_query_bias.npy\n",
      "✅ Saved encoder.layer.4.attention.self.key.weight to minilm_weights/encoder_layer_4_attention_self_key_weight.npy\n",
      "✅ Saved encoder.layer.4.attention.self.key.bias to minilm_weights/encoder_layer_4_attention_self_key_bias.npy\n",
      "✅ Saved encoder.layer.4.attention.self.value.weight to minilm_weights/encoder_layer_4_attention_self_value_weight.npy\n",
      "✅ Saved encoder.layer.4.attention.self.value.bias to minilm_weights/encoder_layer_4_attention_self_value_bias.npy\n",
      "✅ Saved encoder.layer.4.attention.output.dense.weight to minilm_weights/encoder_layer_4_attention_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.4.attention.output.dense.bias to minilm_weights/encoder_layer_4_attention_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.4.attention.output.LayerNorm.weight to minilm_weights/encoder_layer_4_attention_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.4.attention.output.LayerNorm.bias to minilm_weights/encoder_layer_4_attention_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.4.intermediate.dense.weight to minilm_weights/encoder_layer_4_intermediate_dense_weight.npy\n",
      "✅ Saved encoder.layer.4.intermediate.dense.bias to minilm_weights/encoder_layer_4_intermediate_dense_bias.npy\n",
      "✅ Saved encoder.layer.4.output.dense.weight to minilm_weights/encoder_layer_4_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.4.output.dense.bias to minilm_weights/encoder_layer_4_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.4.output.LayerNorm.weight to minilm_weights/encoder_layer_4_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.4.output.LayerNorm.bias to minilm_weights/encoder_layer_4_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.5.attention.self.query.weight to minilm_weights/encoder_layer_5_attention_self_query_weight.npy\n",
      "✅ Saved encoder.layer.5.attention.self.query.bias to minilm_weights/encoder_layer_5_attention_self_query_bias.npy\n",
      "✅ Saved encoder.layer.5.attention.self.key.weight to minilm_weights/encoder_layer_5_attention_self_key_weight.npy\n",
      "✅ Saved encoder.layer.5.attention.self.key.bias to minilm_weights/encoder_layer_5_attention_self_key_bias.npy\n",
      "✅ Saved encoder.layer.5.attention.self.value.weight to minilm_weights/encoder_layer_5_attention_self_value_weight.npy\n",
      "✅ Saved encoder.layer.5.attention.self.value.bias to minilm_weights/encoder_layer_5_attention_self_value_bias.npy\n",
      "✅ Saved encoder.layer.5.attention.output.dense.weight to minilm_weights/encoder_layer_5_attention_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.5.attention.output.dense.bias to minilm_weights/encoder_layer_5_attention_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.5.attention.output.LayerNorm.weight to minilm_weights/encoder_layer_5_attention_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.5.attention.output.LayerNorm.bias to minilm_weights/encoder_layer_5_attention_output_LayerNorm_bias.npy\n",
      "✅ Saved encoder.layer.5.intermediate.dense.weight to minilm_weights/encoder_layer_5_intermediate_dense_weight.npy\n",
      "✅ Saved encoder.layer.5.intermediate.dense.bias to minilm_weights/encoder_layer_5_intermediate_dense_bias.npy\n",
      "✅ Saved encoder.layer.5.output.dense.weight to minilm_weights/encoder_layer_5_output_dense_weight.npy\n",
      "✅ Saved encoder.layer.5.output.dense.bias to minilm_weights/encoder_layer_5_output_dense_bias.npy\n",
      "✅ Saved encoder.layer.5.output.LayerNorm.weight to minilm_weights/encoder_layer_5_output_LayerNorm_weight.npy\n",
      "✅ Saved encoder.layer.5.output.LayerNorm.bias to minilm_weights/encoder_layer_5_output_LayerNorm_bias.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set output folder\n",
    "os.makedirs(\"minilm_weights\", exist_ok=True)\n",
    "\n",
    "# Extract weights from all 6 layers\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder.layer\"):\n",
    "        filename = f\"minilm_weights/{name.replace('.', '_')}.npy\"\n",
    "        np.save(filename, param.detach().cpu().numpy())\n",
    "        print(f\"✅ Saved {name} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffa027-9a67-495e-af87-0be4c856c88c",
   "metadata": {},
   "source": [
    "## Step 10: Let's get Q, K and V matrices from within MiniLM\n",
    "Products of input and query projection + query bias, key projection + key bias, value projection + value bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b9177-0e5f-48e1-920d-30581549498c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a15a10e4-123f-4448-856b-332dfda8046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_ref shape: (9, 384)\n",
      "K_ref shape: (9, 384)\n",
      "V_ref shape: (9, 384)\n",
      "\n",
      "Q_ref[0][:5]: [-0.2699886  -0.53702736 -0.04728177  1.364982   -0.08339243]\n",
      "K_ref[0][:5]: [-0.08351308 -0.16842997  0.2463154  -0.11964889 -0.06481847]\n",
      "V_ref[0][:5]: [ 0.26878867 -0.09818755 -0.09449198  0.16401815 -0.0027703 ]\n",
      "\n",
      "✅ Saved MiniLM Layer 0 Q, K, V as CSV files.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Get Layer 0 of MiniLM\n",
    "    layer0 = model.encoder.layer[0]\n",
    "    input_pt = manual_input.clone()  # shape: [1, seq_len, 384]\n",
    "\n",
    "    # Compute MiniLM's internal projections\n",
    "    Q_ref = layer0.attention.self.query(input_pt).squeeze(0).cpu().numpy()\n",
    "    K_ref = layer0.attention.self.key(input_pt).squeeze(0).cpu().numpy()\n",
    "    V_ref = layer0.attention.self.value(input_pt).squeeze(0).cpu().numpy()\n",
    "\n",
    "# ✅ Display shapes and sample values\n",
    "print(\"Q_ref shape:\", Q_ref.shape)\n",
    "print(\"K_ref shape:\", K_ref.shape)\n",
    "print(\"V_ref shape:\", V_ref.shape)\n",
    "\n",
    "print(\"\\nQ_ref[0][:5]:\", Q_ref[0][:5])\n",
    "print(\"K_ref[0][:5]:\", K_ref[0][:5])\n",
    "print(\"V_ref[0][:5]:\", V_ref[0][:5])\n",
    "\n",
    "# ✅ Save to CSV\n",
    "pd.DataFrame(Q_ref, index=[f\"token_{i}\" for i in range(Q_ref.shape[0])]).to_csv(\n",
    "    \"minilm_layer0_Q_ref.csv\", index_label=\"token\"\n",
    ")\n",
    "pd.DataFrame(K_ref, index=[f\"token_{i}\" for i in range(K_ref.shape[0])]).to_csv(\n",
    "    \"minilm_layer0_K_ref.csv\", index_label=\"token\"\n",
    ")\n",
    "pd.DataFrame(V_ref, index=[f\"token_{i}\" for i in range(V_ref.shape[0])]).to_csv(\n",
    "    \"minilm_layer0_V_ref.csv\", index_label=\"token\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Saved MiniLM Layer 0 Q, K, V as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d2852-7278-4cef-b0de-a1f9a88f9b33",
   "metadata": {},
   "source": [
    "## Step 10a: Now let's compute layer one Q, K, V by calling the weights and input matrix and compare to MiniLM\n",
    "This section shows that our computed matrices are an excellent match for MiniLM's internal computations for layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50e1cc2b-41a7-46a9-9016-647049cfde7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Max abs diff (Q): 1.66893e-06\n",
      "🔍 Max abs diff (K): 7.748604e-07\n",
      "🔍 Max abs diff (V): 7.1525574e-07\n"
     ]
    }
   ],
   "source": [
    "# Ensure manual_input is converted to NumPy (shape [6, 384])\n",
    "if isinstance(manual_input, torch.Tensor):\n",
    "    manual_input_np = manual_input.squeeze(0).detach().cpu().numpy()\n",
    "else:\n",
    "    manual_input_np = manual_input  # already in NumPy\n",
    "\n",
    "# Load Layer 0 weights and biases\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_0_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_0_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_0_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_0_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_0_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_0_attention_self_value_bias.npy\")\n",
    "\n",
    "# Compute manually\n",
    "Q_manual = manual_input_np @ W_q.T + b_q\n",
    "K_manual = manual_input_np @ W_k.T + b_k\n",
    "V_manual = manual_input_np @ W_v.T + b_v\n",
    "\n",
    "# Compare with MiniLM outputs\n",
    "print(\"🔍 Max abs diff (Q):\", np.abs(Q_manual - Q_ref).max())\n",
    "print(\"🔍 Max abs diff (K):\", np.abs(K_manual - K_ref).max())\n",
    "print(\"🔍 Max abs diff (V):\", np.abs(V_manual - V_ref).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e759b36-ae44-47d0-a567-db61f4b3d22b",
   "metadata": {},
   "source": [
    "## Step 12: Now let's get the self attention scores for block 1 from MiniLM for our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12136f79-a174-4a5c-91a0-a54bcfd6da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MiniLM Layer 0 – Head 0 Attention Scores (6×6):\n",
      "[[-0.1226  0.0408  0.1573  0.098   0.1382  0.0791  0.0524  0.1343  0.1992]\n",
      " [ 0.2471  0.042   0.042   0.0346  0.0326  0.0173  0.0275  0.0554  0.116 ]\n",
      " [ 0.1513  0.062   0.0665  0.0764  0.0786  0.0323  0.0475  0.0855  0.1372]\n",
      " [ 0.2583  0.0438  0.0421  0.049   0.0504 -0.0035  0.0319  0.071   0.118 ]\n",
      " [ 0.2206  0.0667  0.0487  0.0688  0.0369  0.0106  0.0281  0.069   0.1163]\n",
      " [ 0.1824  0.0296  0.0834  0.0466  0.065   0.0128  0.0537  0.071   0.132 ]\n",
      " [ 0.2376  0.0521  0.0478  0.0506  0.034   0.008   0.0324  0.0675  0.1138]\n",
      " [ 0.1562  0.0636  0.0961  0.0727  0.0865  0.0394  0.0501  0.1175  0.1602]\n",
      " [ 0.0914  0.0647  0.0942  0.075   0.095   0.0417  0.0638  0.1002  0.1547]]\n",
      "\n",
      "✅ Saved attention scores for all 12 heads (minilm_layer0_attention_scores_head*.csv).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with torch.no_grad():\n",
    "    layer0 = model.encoder.layer[0]\n",
    "    input_pt = manual_input.clone()  # [1, seq_len, 384]\n",
    "\n",
    "    # Get Q, K projections\n",
    "    Q = layer0.attention.self.query(input_pt)  # [1, 6, 384]\n",
    "    K = layer0.attention.self.key(input_pt)    # [1, 6, 384]\n",
    "\n",
    "    # Split into heads: [batch, num_heads, seq_len, head_dim]\n",
    "    def split_heads(x):\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        num_heads = 12\n",
    "        head_dim = hidden_dim // num_heads\n",
    "        return x.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "    Q_split = split_heads(Q)  # [1, 12, 6, 32]\n",
    "    K_split = split_heads(K)  # [1, 12, 6, 32]\n",
    "\n",
    "    # Compute scaled dot-product attention scores: Q @ K^T / sqrt(d_k)\n",
    "    d_k = Q_split.size(-1)\n",
    "    scores = torch.matmul(Q_split, K_split.transpose(-2, -1)) / np.sqrt(d_k)  # [1, 12, 6, 6]\n",
    "    scores_np = scores.squeeze(0).cpu().numpy()  # [12, 6, 6]\n",
    "\n",
    "# Display attention scores from head 0\n",
    "print(\"✅ MiniLM Layer 0 – Head 0 Attention Scores (6×6):\")\n",
    "print(np.round(scores_np[0], 4))\n",
    "\n",
    "# Save all 12 heads to CSV\n",
    "for h in range(12):\n",
    "    pd.DataFrame(scores_np[h]).to_csv(f\"minilm_layer0_attention_scores_head{h}.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Saved attention scores for all 12 heads (minilm_layer0_attention_scores_head*.csv).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c403fb-5378-4dfa-a8a3-1991a3d121d1",
   "metadata": {},
   "source": [
    "## Step 12 a: Manually calcualte the attention scores for block one\n",
    "Manually compute the scaled dot-product attention score matrices for all 12 heads in MiniLM Layer 0 (Block 1), using Q and K projections derived from manual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11b0344d-0105-419c-988a-399e749eac44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual attention score calculation — Layer 0 with 12 heads (32 dim each)\n",
      "\n",
      "Head 0 – Manual Attention Score Matrix (rounded):\n",
      "[[-0.1226  0.0408  0.1573  0.098   0.1382  0.0791  0.0524  0.1343  0.1992]\n",
      " [ 0.2471  0.042   0.042   0.0346  0.0326  0.0173  0.0275  0.0554  0.116 ]\n",
      " [ 0.1513  0.062   0.0665  0.0764  0.0786  0.0323  0.0475  0.0855  0.1372]\n",
      " [ 0.2583  0.0438  0.0421  0.049   0.0504 -0.0035  0.0319  0.071   0.118 ]\n",
      " [ 0.2206  0.0667  0.0487  0.0688  0.0369  0.0106  0.0281  0.069   0.1163]\n",
      " [ 0.1824  0.0296  0.0834  0.0466  0.065   0.0128  0.0537  0.071   0.132 ]\n",
      " [ 0.2376  0.0521  0.0478  0.0506  0.034   0.008   0.0324  0.0675  0.1138]\n",
      " [ 0.1562  0.0636  0.0961  0.0727  0.0865  0.0394  0.0501  0.1175  0.1602]\n",
      " [ 0.0914  0.0647  0.0942  0.075   0.095   0.0417  0.0638  0.1002  0.1547]]\n",
      "\n",
      "Head 1 – Manual Attention Score Matrix (rounded):\n",
      "[[ 0.1205  0.0479 -0.0659 -0.0202  0.0075 -0.0228 -0.0808 -0.0076 -0.0041]\n",
      " [ 0.2827  0.0527  0.01    0.0491  0.0069 -0.0166  0.0466  0.0081  0.0664]\n",
      " [ 0.2819  0.0162 -0.0136  0.0169  0.0168 -0.0125  0.0176 -0.018   0.0413]\n",
      " [ 0.2835  0.0217  0.0136  0.0426  0.0127 -0.007   0.0159 -0.0011  0.0455]\n",
      " [ 0.2824  0.0099  0.0014  0.0162  0.0144 -0.0071 -0.0021 -0.0161  0.0396]\n",
      " [ 0.3248  0.0024  0.0373 -0.0024  0.0366  0.0352  0.0059  0.0027  0.0719]\n",
      " [ 0.3337  0.0231  0.0471  0.0187  0.0301  0.0065  0.0687  0.0337  0.0962]\n",
      " [ 0.2836  0.0121 -0.0052  0.0204  0.0251 -0.0032  0.0149 -0.0155  0.0446]\n",
      " [ 0.3004  0.0195  0.0038  0.0318  0.023  -0.001   0.0118 -0.0026  0.0469]]\n",
      "\n",
      "Head 2 – Manual Attention Score Matrix (rounded):\n",
      "[[ 3.9276e+00  5.3600e-02 -1.9100e-01  4.4000e-03 -2.2780e-01 -8.7800e-02\n",
      "   1.6780e-01 -3.3370e-01 -2.5800e-01]\n",
      " [ 4.4477e+00  3.2200e-01 -9.3800e-02  9.6000e-02 -1.7810e-01  1.3820e-01\n",
      "   3.2950e-01 -4.3600e-01 -4.8190e-01]\n",
      " [ 3.5123e+00  4.5740e-01  9.7600e-02  2.0550e-01 -3.1270e-01  2.1000e-02\n",
      "   4.0610e-01 -2.1360e-01 -3.7670e-01]\n",
      " [ 3.2829e+00  3.4130e-01  2.8550e-01  4.6820e-01 -1.7490e-01 -9.5300e-02\n",
      "   2.3900e-01 -1.2150e-01 -6.2500e-02]\n",
      " [ 3.6813e+00  1.2500e-01  1.1510e-01  6.0080e-01  2.0300e-02  3.8600e-02\n",
      "   1.1780e-01 -3.0570e-01 -8.1700e-02]\n",
      " [ 3.3802e+00  5.1600e-02 -1.8680e-01  4.2590e-01  1.3620e-01  2.2920e-01\n",
      "   1.9960e-01 -4.5890e-01 -2.7920e-01]\n",
      " [ 3.7636e+00  2.7100e-01 -1.0890e-01  1.8730e-01  7.6000e-03  3.6310e-01\n",
      "   4.5750e-01 -3.0300e-01 -3.4780e-01]\n",
      " [ 3.5789e+00  3.1180e-01 -2.3600e-02  1.2870e-01 -2.8690e-01  1.7570e-01\n",
      "   5.4210e-01 -1.0230e-01 -2.5510e-01]\n",
      " [ 3.7555e+00  2.0330e-01  1.2200e-01  3.2770e-01 -3.1480e-01 -4.5400e-02\n",
      "   3.9040e-01  4.3400e-02 -3.6600e-02]]\n",
      "\n",
      "Head 3 – Manual Attention Score Matrix (rounded):\n",
      "[[ 0.2919 -0.0083  0.0796  0.0668 -0.0578  0.0117  0.142   0.1913  0.129 ]\n",
      " [-0.1475  0.0028 -0.1005  0.0356 -0.0453 -0.026   0.0339 -0.0254 -0.0936]\n",
      " [-0.0513  0.0445 -0.0072  0.0666  0.0103  0.0202  0.0555  0.0316 -0.0205]\n",
      " [-0.2131  0.038  -0.0526  0.0431 -0.0109  0.0043  0.0495  0.0017 -0.0795]\n",
      " [-0.0714  0.017  -0.0691  0.0519 -0.0038  0.0164  0.0285 -0.0109 -0.0552]\n",
      " [-0.0387  0.0305 -0.0432  0.043  -0.0019  0.0091  0.0408  0.0097 -0.0383]\n",
      " [-0.1212  0.0123 -0.1066  0.0445 -0.0454 -0.0091  0.0321 -0.0396 -0.0904]\n",
      " [ 0.0229  0.0396 -0.032   0.0572  0.0095  0.0259  0.0551  0.0353 -0.0028]\n",
      " [ 0.0413  0.0357 -0.0216  0.0668  0.0105  0.033   0.0601  0.0443 -0.0009]]\n",
      "\n",
      "Head 4 – Manual Attention Score Matrix (rounded):\n",
      "[[ 1.499   0.1568  0.3554  0.2267  0.2166 -0.0308  0.1965  0.3436  0.3697]\n",
      " [ 1.0788  0.1247  0.2479  0.2338  0.1466 -0.0182  0.0646  0.2281  0.2602]\n",
      " [ 1.106   0.1191  0.259   0.1787  0.1722  0.0155  0.0831  0.2564  0.2729]\n",
      " [ 1.0706  0.1465  0.2684  0.198   0.1688  0.0038  0.0983  0.2435  0.2792]\n",
      " [ 1.0557  0.1216  0.2737  0.1862  0.1629  0.0527  0.0678  0.2603  0.2804]\n",
      " [ 0.9583  0.1149  0.2831  0.19    0.1923  0.0179  0.0933  0.2772  0.2817]\n",
      " [ 1.0436  0.1584  0.2664  0.1392  0.1622  0.0186  0.0818  0.2528  0.2686]\n",
      " [ 1.1347  0.1182  0.266   0.1874  0.1719  0.0134  0.0886  0.2691  0.2995]\n",
      " [ 1.227   0.1299  0.274   0.1864  0.1718 -0.0049  0.0916  0.2781  0.3083]]\n",
      "\n",
      "Head 5 – Manual Attention Score Matrix (rounded):\n",
      "[[1.816  0.0532 0.1923 0.1359 0.0724 0.0882 0.0708 0.2968 0.3116]\n",
      " [1.8573 0.0538 0.1852 0.1425 0.0966 0.0599 0.0836 0.2294 0.2589]\n",
      " [1.7586 0.0677 0.2078 0.1451 0.0998 0.0751 0.0897 0.2514 0.2646]\n",
      " [1.7635 0.1495 0.2258 0.1517 0.1087 0.0627 0.0951 0.2771 0.2905]\n",
      " [1.8247 0.085  0.2369 0.1813 0.1411 0.1088 0.1006 0.2879 0.309 ]\n",
      " [1.6652 0.0079 0.1792 0.1622 0.1326 0.0821 0.0824 0.2338 0.2583]\n",
      " [1.7785 0.0656 0.2067 0.1776 0.1396 0.0937 0.0698 0.2792 0.2934]\n",
      " [1.7607 0.0544 0.1962 0.1418 0.129  0.0993 0.1146 0.2812 0.2904]\n",
      " [1.8586 0.0634 0.2059 0.1436 0.1274 0.1057 0.1054 0.3188 0.326 ]]\n",
      "\n",
      "Head 6 – Manual Attention Score Matrix (rounded):\n",
      "[[ 0.37    0.064   0.0714  0.1093  0.0595 -0.0312  0.0439  0.0864  0.1295]\n",
      " [ 0.2263 -0.0077 -0.0548 -0.0051 -0.0175 -0.056   0.002  -0.0491  0.0251]\n",
      " [ 0.2885  0.0222  0.0048  0.0354  0.0027 -0.0416  0.0036 -0.0389  0.0504]\n",
      " [ 0.2626 -0.0111 -0.0645  0.0141 -0.024  -0.0836 -0.0143 -0.0791  0.0219]\n",
      " [ 0.2471 -0.0098 -0.0665 -0.0057 -0.0104 -0.0711 -0.0175 -0.0596  0.0152]\n",
      " [ 0.2323  0.0052 -0.0542  0.0089 -0.0143 -0.0807  0.0093 -0.0471  0.0151]\n",
      " [ 0.2206 -0.0004 -0.0539  0.0163 -0.0237 -0.068  -0.0113 -0.0679  0.021 ]\n",
      " [ 0.2663  0.0062 -0.0456  0.0197 -0.0037 -0.0483 -0.0017 -0.018   0.0488]\n",
      " [ 0.2792  0.0199 -0.0112  0.0328  0.0075 -0.0426 -0.0016 -0.031   0.0505]]\n",
      "\n",
      "Head 7 – Manual Attention Score Matrix (rounded):\n",
      "[[ 0.3765  0.0263  0.0419  0.0312  0.0195  0.026   0.0215  0.0384  0.0735]\n",
      " [ 0.366  -0.0128  0.0326 -0.0039 -0.0139 -0.023   0.0412  0.0204  0.0737]\n",
      " [ 0.3372  0.0023  0.0328  0.0018  0.0071 -0.0176  0.0352  0.0254  0.0699]\n",
      " [ 0.3556 -0.0049  0.0258 -0.034   0.0084 -0.0261  0.0329  0.0139  0.0701]\n",
      " [ 0.3424  0.0051  0.0352  0.0024 -0.0019 -0.015   0.0313  0.0262  0.0708]\n",
      " [ 0.3631  0.0043  0.0388 -0.0032 -0.0049 -0.0245  0.0418  0.0227  0.0779]\n",
      " [ 0.3863  0.02    0.0282 -0.0094 -0.0061 -0.008   0.0091  0.0222  0.0777]\n",
      " [ 0.3427  0.0098  0.0324 -0.0018  0.0123 -0.015   0.0314  0.0249  0.0689]\n",
      " [ 0.3591  0.0077  0.0339  0.0062  0.0083 -0.0098  0.03    0.0272  0.0721]]\n",
      "\n",
      "Head 8 – Manual Attention Score Matrix (rounded):\n",
      "[[ 9.891e-01 -3.280e-02  1.299e-01  1.139e-01 -5.880e-02 -6.040e-02\n",
      "   1.031e-01  1.219e-01  2.187e-01]\n",
      " [ 4.590e-01  3.900e-03  3.100e-03 -3.900e-03 -1.630e-02 -8.190e-02\n",
      "  -3.690e-02 -1.760e-02  6.860e-02]\n",
      " [ 4.725e-01 -3.150e-02  1.590e-02  1.840e-02 -8.500e-03 -6.440e-02\n",
      "  -9.300e-03  9.700e-03  7.840e-02]\n",
      " [ 4.773e-01 -2.500e-02  1.420e-02  4.800e-02 -1.510e-02 -6.970e-02\n",
      "  -2.610e-02  4.600e-03  8.330e-02]\n",
      " [ 4.708e-01 -2.080e-02  6.600e-03  2.500e-02 -5.000e-04 -8.480e-02\n",
      "  -2.810e-02 -1.120e-02  6.900e-02]\n",
      " [ 4.632e-01 -2.370e-02  8.700e-03  1.990e-02 -5.500e-03 -5.900e-02\n",
      "  -3.230e-02 -1.900e-03  6.430e-02]\n",
      " [ 5.096e-01 -1.620e-02  1.720e-02  2.420e-02 -1.840e-02 -8.490e-02\n",
      "   2.590e-02  7.500e-03  9.930e-02]\n",
      " [ 4.484e-01 -3.630e-02  2.300e-03  1.280e-02 -1.350e-02 -7.890e-02\n",
      "  -2.740e-02 -1.140e-02  6.690e-02]\n",
      " [ 5.209e-01 -1.530e-02  2.630e-02  2.630e-02  6.200e-03 -6.220e-02\n",
      "  -1.140e-02  1.470e-02  9.330e-02]]\n",
      "\n",
      "Head 9 – Manual Attention Score Matrix (rounded):\n",
      "[[-5.208e-01  2.380e-02 -1.061e-01  6.840e-02 -1.737e-01 -2.810e-02\n",
      "   7.440e-02  5.100e-02  8.040e-02]\n",
      " [ 2.652e-01 -1.390e-02 -4.870e-02  1.910e-02 -2.550e-02 -7.070e-02\n",
      "  -3.290e-02 -2.188e-01 -2.050e-02]\n",
      " [ 3.002e-01 -2.540e-02 -5.230e-02  1.230e-02 -2.190e-02 -7.320e-02\n",
      "  -4.120e-02 -2.143e-01 -1.060e-02]\n",
      " [ 3.235e-01 -2.250e-02 -2.970e-02  9.700e-03 -7.200e-03 -7.520e-02\n",
      "  -2.940e-02 -2.050e-01  1.280e-02]\n",
      " [ 3.369e-01 -3.060e-02 -3.260e-02  1.620e-02 -9.200e-03 -6.140e-02\n",
      "  -3.320e-02 -2.120e-01 -1.000e-04]\n",
      " [ 3.455e-01 -2.720e-02 -3.550e-02  6.700e-03 -7.500e-03 -5.850e-02\n",
      "  -4.040e-02 -2.211e-01 -9.100e-03]\n",
      " [ 3.190e-01 -2.970e-02 -3.920e-02  8.600e-03 -1.220e-02 -6.330e-02\n",
      "  -3.480e-02 -2.180e-01 -1.240e-02]\n",
      " [ 3.109e-01 -3.420e-02 -5.060e-02  1.250e-02 -2.290e-02 -7.450e-02\n",
      "  -4.600e-02 -2.221e-01 -1.770e-02]\n",
      " [ 3.414e-01 -1.990e-02 -2.850e-02  2.250e-02 -1.550e-02 -6.010e-02\n",
      "  -2.820e-02 -1.895e-01  1.430e-02]]\n",
      "\n",
      "Head 10 – Manual Attention Score Matrix (rounded):\n",
      "[[ 3.635e-01 -4.600e-03  1.032e-01  1.145e-01  1.338e-01 -1.770e-02\n",
      "   4.310e-02  1.744e-01  2.425e-01]\n",
      " [ 2.386e-01 -4.400e-03  4.390e-02  1.490e-02  7.070e-02 -2.560e-02\n",
      "  -4.400e-03  4.040e-02  1.239e-01]\n",
      " [ 2.206e-01 -8.900e-03  7.890e-02  5.140e-02  9.630e-02  1.230e-02\n",
      "   3.270e-02  9.120e-02  1.683e-01]\n",
      " [ 2.395e-01  6.000e-04  4.980e-02  1.390e-02  7.640e-02 -3.120e-02\n",
      "   2.780e-02  3.070e-02  1.237e-01]\n",
      " [ 2.580e-01 -1.270e-02  6.060e-02  3.140e-02  9.050e-02 -1.590e-02\n",
      "   1.850e-02  4.940e-02  1.439e-01]\n",
      " [ 2.471e-01  1.100e-03  6.040e-02  1.990e-02  7.710e-02 -1.020e-02\n",
      "   8.300e-03  5.300e-02  1.384e-01]\n",
      " [ 2.933e-01  3.400e-02  6.910e-02 -1.000e-04  8.690e-02 -3.390e-02\n",
      "   5.610e-02  6.370e-02  1.465e-01]\n",
      " [ 2.613e-01  1.050e-02  8.580e-02  4.000e-02  1.056e-01 -6.600e-03\n",
      "   2.740e-02  8.150e-02  1.699e-01]\n",
      " [ 2.871e-01 -4.300e-03  8.480e-02  5.050e-02  9.250e-02 -6.000e-03\n",
      "   2.530e-02  8.920e-02  1.755e-01]]\n",
      "\n",
      "Head 11 – Manual Attention Score Matrix (rounded):\n",
      "[[ 4.723e-01 -2.170e-02 -6.000e-03 -2.500e-02  1.190e-02 -3.550e-02\n",
      "  -1.020e-02  1.590e-02  2.610e-02]\n",
      " [ 2.818e-01 -1.210e-02 -2.100e-03  5.600e-03  2.450e-02 -1.700e-03\n",
      "   9.100e-03  1.280e-02  1.440e-02]\n",
      " [ 2.098e-01 -1.860e-02 -1.050e-02  7.000e-04  2.050e-02 -2.950e-02\n",
      "  -2.100e-02 -4.500e-03 -3.000e-03]\n",
      " [ 2.070e-01 -1.920e-02 -1.440e-02 -6.800e-03  1.960e-02 -3.240e-02\n",
      "  -1.500e-03 -1.750e-02  3.100e-03]\n",
      " [ 2.276e-01 -1.640e-02 -3.000e-04  1.700e-02  3.070e-02 -1.190e-02\n",
      "  -9.300e-03  4.100e-03  5.800e-03]\n",
      " [ 2.345e-01 -1.820e-02 -1.030e-02  4.700e-03  2.710e-02 -1.790e-02\n",
      "  -1.960e-02 -4.100e-03  4.500e-03]\n",
      " [ 2.019e-01 -8.100e-03 -1.060e-02  1.500e-02  1.960e-02 -2.690e-02\n",
      "  -1.130e-02 -7.300e-03 -1.060e-02]\n",
      " [ 1.629e-01 -1.300e-02 -1.150e-02  2.600e-03  1.900e-02 -2.020e-02\n",
      "  -1.100e-02 -1.700e-03 -6.000e-03]\n",
      " [ 1.967e-01 -6.200e-03 -6.100e-03  1.300e-03  2.000e-02 -2.160e-02\n",
      "  -8.400e-03  2.200e-03 -1.200e-03]]\n",
      "\n",
      "✅ Completed manual attention score calculation and saved all 12 matrices.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Setup: dimensions and splitting logic\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "seq_len = Q_manual.shape[0]       # Number of tokens in sequence (e.g., 6)\n",
    "hidden_dim = Q_manual.shape[1]    # Full hidden dimension (e.g., 384 for MiniLM)\n",
    "num_heads = 12                    # Number of attention heads in MiniLM\n",
    "head_dim = hidden_dim // num_heads  # 384 / 12 = 32 dims per head\n",
    "\n",
    "print(f\"Manual attention score calculation — Layer 0 with {num_heads} heads ({head_dim} dim each)\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 1: Split Q and K into per-head slices (12 total)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)  # List of [6, 32]\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 2: For each head, compute scaled dot-product attention scores\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "manual_scores = []  # Will hold one 6x6 score matrix per head\n",
    "\n",
    "for i in range(num_heads):\n",
    "    Qh = Q_heads[i]  # Q for head i: shape [6, 32]\n",
    "    Kh = K_heads[i]  # K for head i: shape [6, 32]\n",
    "\n",
    "    # Raw dot product between each Q and all K (token-to-token interaction)\n",
    "    raw_scores = Qh @ Kh.T  # shape: [6, 6]\n",
    "\n",
    "    # Scale by sqrt(head_dim) as in standard transformer attention\n",
    "    scaled_scores = raw_scores / np.sqrt(head_dim)  # scale by sqrt(32)\n",
    "\n",
    "    manual_scores.append(scaled_scores)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Step 3: Display and save this head's score matrix\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\nHead {i} – Manual Attention Score Matrix (rounded):\")\n",
    "    print(np.round(scaled_scores, 4))\n",
    "\n",
    "    pd.DataFrame(scaled_scores).to_csv(\n",
    "        f\"manual_layer0_attention_scores_head{i}.csv\", index=False\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Completion message\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"\\n✅ Completed manual attention score calculation and saved all 12 matrices.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04885ced-608d-47f0-a2c1-108ef074e5f7",
   "metadata": {},
   "source": [
    "## Now let's compare MiniLM attention scores for block one and our calculated attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ff74a05-4ea0-43d0-8b94-9ebddc384b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 0: max abs diff = 0.00000008\n",
      "Head 1: max abs diff = 0.00000012\n",
      "Head 2: max abs diff = 0.00000190\n",
      "Head 3: max abs diff = 0.00000013\n",
      "Head 4: max abs diff = 0.00000040\n",
      "Head 5: max abs diff = 0.00000030\n",
      "Head 6: max abs diff = 0.00000009\n",
      "Head 7: max abs diff = 0.00000021\n",
      "Head 8: max abs diff = 0.00000012\n",
      "Head 9: max abs diff = 0.00000020\n",
      "Head 10: max abs diff = 0.00000020\n",
      "Head 11: max abs diff = 0.00000012\n",
      "\n",
      "✅ Completed comparison of manual vs MiniLM attention scores for all heads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 1: Configuration — number of heads\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "num_heads = 12\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 2: Compare each head's score matrix\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "for i in range(num_heads):\n",
    "    # Load MiniLM's attention scores for head i\n",
    "    minilm_scores = np.loadtxt(f\"minilm_layer0_attention_scores_head{i}.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "    # Load your manually computed attention scores for head i\n",
    "    manual_scores = np.loadtxt(f\"manual_layer0_attention_scores_head{i}.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "    # Compute element-wise absolute difference\n",
    "    abs_diff = np.abs(minilm_scores - manual_scores)\n",
    "\n",
    "    # Report the maximum absolute difference\n",
    "    max_diff = abs_diff.max()\n",
    "\n",
    "    print(f\"Head {i}: max abs diff = {max_diff:.8f}\")\n",
    "\n",
    "    # Optional: flag if it's too high\n",
    "    if max_diff > 1e-5:\n",
    "        print(\"⚠️  Warning: possible mismatch in head\", i)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Completion message\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"\\n✅ Completed comparison of manual vs MiniLM attention scores for all heads.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5b515-c106-4ce9-a28f-cd429a80b59e",
   "metadata": {},
   "source": [
    "## Now let's get layer 1 attention outout for our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63af1b22-9f83-4c82-a1fd-f0eb9e515bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MiniLM Layer 0 – Attention Output\n",
      "Shape: (9, 384)\n",
      "First token (first 5 dims): [-0.366862 -0.023811  0.300175 -0.19604   0.2298  ]\n",
      "\n",
      "✅ Saved attention output to minilm_layer0_attention_output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 1: Extract attention output from MiniLM Layer 0 (Block 1)\n",
    "# ---------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    layer0 = model.encoder.layer[0]\n",
    "\n",
    "    # Get attention output (first element of returned tuple)\n",
    "    attn_output_tensor = layer0.attention(manual_input)[0]  # shape: [1, 6, 384]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 2: Convert to NumPy\n",
    "# ---------------------------------------------------------------------\n",
    "attn_output = attn_output_tensor.squeeze(0).cpu().numpy()  # shape: [6, 384]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 3: Display shape and preview a few values\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"✅ MiniLM Layer 0 – Attention Output\")\n",
    "print(\"Shape:\", attn_output.shape)\n",
    "print(\"First token (first 5 dims):\", np.round(attn_output[0][:5], 6))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 4: Save to CSV\n",
    "# ---------------------------------------------------------------------\n",
    "pd.DataFrame(attn_output, index=[f\"token_{i}\" for i in range(attn_output.shape[0])]).to_csv(\n",
    "    \"minilm_layer0_attention_output.csv\", index_label=\"token\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Saved attention output to minilm_layer0_attention_output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06377491-3dc8-418e-87da-fb01eb8157ff",
   "metadata": {},
   "source": [
    "## Step 13: Now let's calculate the self attention ourselves and compare to MiniLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a2d904d-7df8-4e92-8a54-edfac6c207e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Manual self-attention output (pre-projection)\n",
      "Shape: (9, 384)\n",
      "First token (first 5 dims): [-0.027055 -0.045212 -0.039891 -0.002907 -0.01545 ]\n",
      "\n",
      "🔍 Max absolute difference vs MiniLM: 0.00000027\n",
      "✅ Match! Manual self-attention output aligns with MiniLM.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Step 1: Compute Q_manual, K_manual, V_manual (from saved weights)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_0_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_0_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_0_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_0_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_0_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_0_attention_self_value_bias.npy\")\n",
    "\n",
    "Q_manual = manual_input_np @ W_q.T + b_q  # shape: [6, 384]\n",
    "K_manual = manual_input_np @ W_k.T + b_k\n",
    "V_manual = manual_input_np @ W_v.T + b_v\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 2: Hook MiniLM to capture raw multi-head attention output (before projection)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "raw_miniLM_heads = None  # global capture variable\n",
    "\n",
    "def hook_capture_heads(module, input, output):\n",
    "    global raw_miniLM_heads\n",
    "    raw_miniLM_heads = input[0].detach().squeeze(0).cpu().numpy()  # shape: [6, 384]\n",
    "\n",
    "# Register hook on Layer 0 output projection\n",
    "hook_handle = model.encoder.layer[0].attention.output.dense.register_forward_hook(hook_capture_heads)\n",
    "\n",
    "# Trigger the hook by running the attention block\n",
    "with torch.no_grad():\n",
    "    _ = model.encoder.layer[0].attention(manual_input)\n",
    "\n",
    "# Remove the hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 3: Manually compute attention output from Q, K, V\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "num_heads = 12\n",
    "head_dim = Q_manual.shape[1] // num_heads  # 384 / 12 = 32\n",
    "seq_len = Q_manual.shape[0]\n",
    "\n",
    "# Split Q, K, V into 12 heads\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "V_heads = np.split(V_manual, num_heads, axis=-1)\n",
    "\n",
    "# Softmax function (stable)\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Compute per-head outputs\n",
    "head_outputs = []\n",
    "\n",
    "for i in range(num_heads):\n",
    "    Q = Q_heads[i]  # [6, 32]\n",
    "    K = K_heads[i]\n",
    "    V = V_heads[i]\n",
    "\n",
    "    scores = Q @ K.T / np.sqrt(head_dim)  # [6, 6]\n",
    "    weights = softmax(scores)             # [6, 6]\n",
    "    head_output = weights @ V             # [6, 32]\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "# Concatenate all heads: [6, 384]\n",
    "multihead_output = np.concatenate(head_outputs, axis=-1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 4: Compare manual result to MiniLM's captured raw output\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "diff = np.abs(multihead_output - raw_miniLM_heads)\n",
    "max_diff = diff.max()\n",
    "\n",
    "print(\"✅ Manual self-attention output (pre-projection)\")\n",
    "print(\"Shape:\", multihead_output.shape)\n",
    "print(\"First token (first 5 dims):\", np.round(multihead_output[0][:5], 6))\n",
    "print(f\"\\n🔍 Max absolute difference vs MiniLM: {max_diff:.8f}\")\n",
    "\n",
    "if max_diff > 1e-6:\n",
    "    print(\"⚠️  Mismatch — check softmax, V multiplication, or concat axis\")\n",
    "else:\n",
    "    print(\"✅ Match! Manual self-attention output aligns with MiniLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616170d-0c23-4e37-8990-86e44e88da8a",
   "metadata": {},
   "source": [
    "## Step 14: Attention output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6e859aa-04fa-4d7c-a308-748cca80cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Attention Output Projection Check\n",
      "🔍 Max diff: 3.2782555e-07\n",
      "🔍 Mean diff: 4.010255e-08\n",
      "✅ Match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Step 1: Manual projection from multi-head output ----\n",
    "W_o = np.load(\"minilm_weights/encoder_layer_0_attention_output_dense_weight.npy\")\n",
    "b_o = np.load(\"minilm_weights/encoder_layer_0_attention_output_dense_bias.npy\")\n",
    "attn_output_manual = multihead_output @ W_o.T + b_o  # shape: [6, 384]\n",
    "\n",
    "# ---- Step 2: Get true MiniLM projection via hook ----\n",
    "raw_attn_output = None\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global raw_attn_output\n",
    "    raw_attn_output = output.detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "hook_handle = model.encoder.layer[0].attention.output.dense.register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model.encoder.layer[0].attention(manual_input)\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "# ---- Step 3: Compare outputs ----\n",
    "diff = np.abs(attn_output_manual - raw_attn_output)\n",
    "print(\"✅ Attention Output Projection Check\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Match:\", np.allclose(attn_output_manual, raw_attn_output, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4b51f-48db-4fcf-9b64-16665580d02d",
   "metadata": {},
   "source": [
    "## Step 15: Add layer norm 1 for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cbf3e3b-0588-4ce9-8792-6f50d99beda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2: Residual + LayerNorm 1\n",
      "🔍 Max diff: 1.9073486e-06\n",
      "🔍 Mean diff: 1.789185e-08\n",
      "✅ Match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ---- Step 1: Manual LayerNorm computation ----\n",
    "\n",
    "gamma1 = np.load(\"minilm_weights/encoder_layer_0_attention_output_LayerNorm_weight.npy\")\n",
    "beta1  = np.load(\"minilm_weights/encoder_layer_0_attention_output_LayerNorm_bias.npy\")\n",
    "\n",
    "ln1_input = attn_output_manual + manual_input_np\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var  = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln1_out_manual = layer_norm(ln1_input, gamma1, beta1)\n",
    "\n",
    "# ---- Step 2: Use LayerNorm module directly ----\n",
    "\n",
    "with torch.no_grad():\n",
    "    ln1_output_minilm = model.encoder.layer[0].attention.output.LayerNorm(\n",
    "        torch.tensor(ln1_input).unsqueeze(0)\n",
    "    ).squeeze(0).numpy()\n",
    "\n",
    "# ---- Step 3: Compare ----\n",
    "\n",
    "diff = np.abs(ln1_out_manual - ln1_output_minilm)\n",
    "print(\"✅ Step 2: Residual + LayerNorm 1\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Match:\", np.allclose(ln1_out_manual, ln1_output_minilm, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a31f36-18c7-4499-be1e-2ba848740ea7",
   "metadata": {},
   "source": [
    "## Step 16: Add layer 2 Norm\n",
    "LayerNorm 2 adds the transformed result (FFN) back to the original LayerNorm 1 output, and then normalizes the sum, keeping values stable for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e84a07c-1b7d-4dc3-950a-9a5a21392244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 4: Residual + LayerNorm 2\n",
      "🔍 Max diff: 4.7683716e-07\n",
      "🔍 Mean diff: 1.3718549e-08\n",
      "✅ Match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Use the ln1_out_manual from your previous calculation (Step 15)\n",
    "# ln1_out_manual is already defined from your previous cell\n",
    "\n",
    "# Calculate feed-forward network output\n",
    "W_intermediate = np.load(\"minilm_weights/encoder_layer_0_intermediate_dense_weight.npy\")\n",
    "b_intermediate = np.load(\"minilm_weights/encoder_layer_0_intermediate_dense_bias.npy\")\n",
    "W_output = np.load(\"minilm_weights/encoder_layer_0_output_dense_weight.npy\")\n",
    "b_output = np.load(\"minilm_weights/encoder_layer_0_output_dense_bias.npy\")\n",
    "\n",
    "# FFN first part (with GELU activation)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "ffn_intermediate = ln1_out_manual @ W_intermediate.T + b_intermediate\n",
    "ffn_intermediate_act = gelu(ffn_intermediate)\n",
    "ff_output_manual = ffn_intermediate_act @ W_output.T + b_output\n",
    "\n",
    "# ---- Step 1: Load LayerNorm 2 weights ----\n",
    "gamma2 = np.load(\"minilm_weights/encoder_layer_0_output_LayerNorm_weight.npy\")\n",
    "beta2 = np.load(\"minilm_weights/encoder_layer_0_output_LayerNorm_bias.npy\")\n",
    "\n",
    "# ---- Step 2: Manual LayerNorm 2 computation ----\n",
    "# Residual connection: add input to FFN output\n",
    "ln2_input = ff_output_manual + ln1_out_manual\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln2_out_manual = layer_norm(ln2_input, gamma2, beta2)\n",
    "\n",
    "# ---- Step 3: Get MiniLM LayerNorm 2 output via module ----\n",
    "with torch.no_grad():\n",
    "    ln2_out_minilm = model.encoder.layer[0].output.LayerNorm(\n",
    "        torch.tensor(ln2_input).unsqueeze(0)\n",
    "    ).squeeze(0).numpy()\n",
    "\n",
    "# ---- Step 4: Compare ----\n",
    "diff = np.abs(ln2_out_manual - ln2_out_minilm)\n",
    "print(\"✅ Step 4: Residual + LayerNorm 2\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Match:\", np.allclose(ln2_out_manual, ln2_out_minilm, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ca06414-607d-4dc1-81c1-c1dbbdda8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83e764b4-290e-4925-b924-c040432fa60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer 1 Complete\n",
      "🔍 Max diff: 1.9073486e-06\n",
      "🔍 Mean diff: 2.3564374e-07\n",
      "✅ Close match (1e-6 tolerance): False\n",
      "✅ Close match (1e-5 tolerance): True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Set up a hook to capture the intermediate outputs from MiniLM ---\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output[0].detach().clone()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on MiniLM layers\n",
    "hooks = []\n",
    "hooks.append(model.encoder.layer[0].register_forward_hook(get_layer_output(\"layer0\")))\n",
    "hooks.append(model.encoder.layer[1].register_forward_hook(get_layer_output(\"layer1\")))\n",
    "\n",
    "# Run forward pass to capture outputs\n",
    "with torch.no_grad():\n",
    "    _ = model(input_ids, attention_mask=torch.ones_like(input_ids))\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Get MiniLM's layer 0 output to use as input to our layer 1\n",
    "layer0_output_minilm = layer_outputs[\"layer0\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "# ----- Layer 1 Implementation -----\n",
    "# 1. Self-Attention Block\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_1_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_1_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_1_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_1_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_1_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_1_attention_self_value_bias.npy\")\n",
    "W_o = np.load(\"minilm_weights/encoder_layer_1_attention_output_dense_weight.npy\")\n",
    "b_o = np.load(\"minilm_weights/encoder_layer_1_attention_output_dense_bias.npy\")\n",
    "\n",
    "# Use MiniLM's layer 0 output as input\n",
    "Q_manual = layer0_output_minilm @ W_q.T + b_q\n",
    "K_manual = layer0_output_minilm @ W_k.T + b_k\n",
    "V_manual = layer0_output_minilm @ W_v.T + b_v\n",
    "\n",
    "# Multi-head attention\n",
    "num_heads = 12\n",
    "head_dim = Q_manual.shape[1] // num_heads\n",
    "seq_len = Q_manual.shape[0]\n",
    "\n",
    "# Split into heads\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "V_heads = np.split(V_manual, num_heads, axis=-1)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "head_outputs = []\n",
    "for i in range(num_heads):\n",
    "    Q = Q_heads[i]\n",
    "    K = K_heads[i]\n",
    "    V = V_heads[i]\n",
    "    \n",
    "    scores = Q @ K.T / np.sqrt(head_dim)\n",
    "    weights = softmax(scores)\n",
    "    head_output = weights @ V\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "multihead_output = np.concatenate(head_outputs, axis=-1)\n",
    "attn_output = multihead_output @ W_o.T + b_o\n",
    "\n",
    "# First Residual + LayerNorm\n",
    "gamma1 = np.load(\"minilm_weights/encoder_layer_1_attention_output_LayerNorm_weight.npy\")\n",
    "beta1 = np.load(\"minilm_weights/encoder_layer_1_attention_output_LayerNorm_bias.npy\")\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln1_input = attn_output + layer0_output_minilm\n",
    "ln1_output = layer_norm(ln1_input, gamma1, beta1)\n",
    "\n",
    "# Feed-Forward Network\n",
    "W_intermediate = np.load(\"minilm_weights/encoder_layer_1_intermediate_dense_weight.npy\")\n",
    "b_intermediate = np.load(\"minilm_weights/encoder_layer_1_intermediate_dense_bias.npy\")\n",
    "W_output = np.load(\"minilm_weights/encoder_layer_1_output_dense_weight.npy\")\n",
    "b_output = np.load(\"minilm_weights/encoder_layer_1_output_dense_bias.npy\")\n",
    "\n",
    "# Use PyTorch's GELU for exact match with MiniLM\n",
    "intermediate = ln1_output @ W_intermediate.T + b_intermediate\n",
    "intermediate_tensor = torch.tensor(intermediate).float()\n",
    "intermediate_act = torch.nn.functional.gelu(intermediate_tensor).numpy()\n",
    "ffn_output = intermediate_act @ W_output.T + b_output\n",
    "\n",
    "# Second Residual + LayerNorm\n",
    "gamma2 = np.load(\"minilm_weights/encoder_layer_1_output_LayerNorm_weight.npy\")\n",
    "beta2 = np.load(\"minilm_weights/encoder_layer_1_output_LayerNorm_bias.npy\")\n",
    "\n",
    "ln2_input = ffn_output + ln1_output\n",
    "layer1_output = layer_norm(ln2_input, gamma2, beta2)\n",
    "\n",
    "# Compare with MiniLM's layer 1 output\n",
    "layer1_output_minilm = layer_outputs[\"layer1\"].squeeze(0).cpu().numpy()\n",
    "diff = np.abs(layer1_output - layer1_output_minilm)\n",
    "\n",
    "# Print additional info\n",
    "print(\"✅ Layer 1 Complete\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Close match (1e-6 tolerance):\", np.allclose(layer1_output, layer1_output_minilm, atol=1e-6))\n",
    "print(\"✅ Close match (1e-5 tolerance):\", np.allclose(layer1_output, layer1_output_minilm, atol=1e-5))\n",
    "\n",
    "# For any future operations, consider this output valid if max diff < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6929fbf-aa5f-4d44-ab21-029875aa3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer 2 Complete\n",
      "🔍 Max diff: 1.9073486e-06\n",
      "🔍 Mean diff: 1.9157926e-07\n",
      "✅ Close match (1e-6 tolerance): True\n",
      "✅ Close match (1e-5 tolerance): True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Set up a hook to capture the intermediate outputs from MiniLM ---\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output[0].detach().clone()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on MiniLM layers\n",
    "hooks = []\n",
    "hooks.append(model.encoder.layer[1].register_forward_hook(get_layer_output(\"layer1\")))\n",
    "hooks.append(model.encoder.layer[2].register_forward_hook(get_layer_output(\"layer2\")))\n",
    "\n",
    "# Run forward pass to capture outputs\n",
    "with torch.no_grad():\n",
    "    _ = model(input_ids, attention_mask=torch.ones_like(input_ids))\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Get MiniLM's layer 1 output to use as input to our layer 2\n",
    "layer1_output_minilm = layer_outputs[\"layer1\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "# ----- Layer 2 Implementation -----\n",
    "# 1. Self-Attention Block\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_2_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_2_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_2_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_2_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_2_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_2_attention_self_value_bias.npy\")\n",
    "W_o = np.load(\"minilm_weights/encoder_layer_2_attention_output_dense_weight.npy\")\n",
    "b_o = np.load(\"minilm_weights/encoder_layer_2_attention_output_dense_bias.npy\")\n",
    "\n",
    "# Use MiniLM's layer 1 output as input\n",
    "Q_manual = layer1_output_minilm @ W_q.T + b_q\n",
    "K_manual = layer1_output_minilm @ W_k.T + b_k\n",
    "V_manual = layer1_output_minilm @ W_v.T + b_v\n",
    "\n",
    "# Multi-head attention\n",
    "num_heads = 12\n",
    "head_dim = Q_manual.shape[1] // num_heads\n",
    "seq_len = Q_manual.shape[0]\n",
    "\n",
    "# Split into heads\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "V_heads = np.split(V_manual, num_heads, axis=-1)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "head_outputs = []\n",
    "for i in range(num_heads):\n",
    "    Q = Q_heads[i]\n",
    "    K = K_heads[i]\n",
    "    V = V_heads[i]\n",
    "    \n",
    "    scores = Q @ K.T / np.sqrt(head_dim)\n",
    "    weights = softmax(scores)\n",
    "    head_output = weights @ V\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "multihead_output = np.concatenate(head_outputs, axis=-1)\n",
    "attn_output = multihead_output @ W_o.T + b_o\n",
    "\n",
    "# First Residual + LayerNorm\n",
    "gamma1 = np.load(\"minilm_weights/encoder_layer_2_attention_output_LayerNorm_weight.npy\")\n",
    "beta1 = np.load(\"minilm_weights/encoder_layer_2_attention_output_LayerNorm_bias.npy\")\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln1_input = attn_output + layer1_output_minilm\n",
    "ln1_output = layer_norm(ln1_input, gamma1, beta1)\n",
    "\n",
    "# Feed-Forward Network\n",
    "W_intermediate = np.load(\"minilm_weights/encoder_layer_2_intermediate_dense_weight.npy\")\n",
    "b_intermediate = np.load(\"minilm_weights/encoder_layer_2_intermediate_dense_bias.npy\")\n",
    "W_output = np.load(\"minilm_weights/encoder_layer_2_output_dense_weight.npy\")\n",
    "b_output = np.load(\"minilm_weights/encoder_layer_2_output_dense_bias.npy\")\n",
    "\n",
    "# Use PyTorch's GELU for exact match with MiniLM\n",
    "intermediate = ln1_output @ W_intermediate.T + b_intermediate\n",
    "intermediate_tensor = torch.tensor(intermediate).float()\n",
    "intermediate_act = torch.nn.functional.gelu(intermediate_tensor).numpy()\n",
    "ffn_output = intermediate_act @ W_output.T + b_output\n",
    "\n",
    "# Second Residual + LayerNorm\n",
    "gamma2 = np.load(\"minilm_weights/encoder_layer_2_output_LayerNorm_weight.npy\")\n",
    "beta2 = np.load(\"minilm_weights/encoder_layer_2_output_LayerNorm_bias.npy\")\n",
    "\n",
    "ln2_input = ffn_output + ln1_output\n",
    "layer2_output = layer_norm(ln2_input, gamma2, beta2)\n",
    "\n",
    "# Compare with MiniLM's layer 2 output\n",
    "layer2_output_minilm = layer_outputs[\"layer2\"].squeeze(0).cpu().numpy()\n",
    "diff = np.abs(layer2_output - layer2_output_minilm)\n",
    "\n",
    "print(\"✅ Layer 2 Complete\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Close match (1e-6 tolerance):\", np.allclose(layer2_output, layer2_output_minilm, atol=1e-6))\n",
    "print(\"✅ Close match (1e-5 tolerance):\", np.allclose(layer2_output, layer2_output_minilm, atol=1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a27f81b0-302f-40ed-b3f0-4a69668422e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer 3 Complete\n",
      "🔍 Max diff: 1.1920929e-06\n",
      "🔍 Mean diff: 1.7707889e-07\n",
      "✅ Close match (1e-6 tolerance): True\n",
      "✅ Close match (1e-5 tolerance): True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Set up a hook to capture the intermediate outputs from MiniLM ---\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output[0].detach().clone()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on MiniLM layers\n",
    "hooks = []\n",
    "hooks.append(model.encoder.layer[2].register_forward_hook(get_layer_output(\"layer2\")))\n",
    "hooks.append(model.encoder.layer[3].register_forward_hook(get_layer_output(\"layer3\")))\n",
    "\n",
    "# Run forward pass to capture outputs\n",
    "with torch.no_grad():\n",
    "    _ = model(input_ids, attention_mask=torch.ones_like(input_ids))\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Get MiniLM's layer 2 output to use as input to our layer 3\n",
    "layer2_output_minilm = layer_outputs[\"layer2\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "# ----- Layer 3 Implementation -----\n",
    "# 1. Self-Attention Block\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_3_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_3_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_3_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_3_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_3_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_3_attention_self_value_bias.npy\")\n",
    "W_o = np.load(\"minilm_weights/encoder_layer_3_attention_output_dense_weight.npy\")\n",
    "b_o = np.load(\"minilm_weights/encoder_layer_3_attention_output_dense_bias.npy\")\n",
    "\n",
    "# Use MiniLM's layer 2 output as input\n",
    "Q_manual = layer2_output_minilm @ W_q.T + b_q\n",
    "K_manual = layer2_output_minilm @ W_k.T + b_k\n",
    "V_manual = layer2_output_minilm @ W_v.T + b_v\n",
    "\n",
    "# Multi-head attention\n",
    "num_heads = 12\n",
    "head_dim = Q_manual.shape[1] // num_heads\n",
    "seq_len = Q_manual.shape[0]\n",
    "\n",
    "# Split into heads\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "V_heads = np.split(V_manual, num_heads, axis=-1)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "head_outputs = []\n",
    "for i in range(num_heads):\n",
    "    Q = Q_heads[i]\n",
    "    K = K_heads[i]\n",
    "    V = V_heads[i]\n",
    "    \n",
    "    scores = Q @ K.T / np.sqrt(head_dim)\n",
    "    weights = softmax(scores)\n",
    "    head_output = weights @ V\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "multihead_output = np.concatenate(head_outputs, axis=-1)\n",
    "attn_output = multihead_output @ W_o.T + b_o\n",
    "\n",
    "# First Residual + LayerNorm\n",
    "gamma1 = np.load(\"minilm_weights/encoder_layer_3_attention_output_LayerNorm_weight.npy\")\n",
    "beta1 = np.load(\"minilm_weights/encoder_layer_3_attention_output_LayerNorm_bias.npy\")\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln1_input = attn_output + layer2_output_minilm\n",
    "ln1_output = layer_norm(ln1_input, gamma1, beta1)\n",
    "\n",
    "# Feed-Forward Network\n",
    "W_intermediate = np.load(\"minilm_weights/encoder_layer_3_intermediate_dense_weight.npy\")\n",
    "b_intermediate = np.load(\"minilm_weights/encoder_layer_3_intermediate_dense_bias.npy\")\n",
    "W_output = np.load(\"minilm_weights/encoder_layer_3_output_dense_weight.npy\")\n",
    "b_output = np.load(\"minilm_weights/encoder_layer_3_output_dense_bias.npy\")\n",
    "\n",
    "# Use PyTorch's GELU for exact match with MiniLM\n",
    "intermediate = ln1_output @ W_intermediate.T + b_intermediate\n",
    "intermediate_tensor = torch.tensor(intermediate).float()\n",
    "intermediate_act = torch.nn.functional.gelu(intermediate_tensor).numpy()\n",
    "ffn_output = intermediate_act @ W_output.T + b_output\n",
    "\n",
    "# Second Residual + LayerNorm\n",
    "gamma2 = np.load(\"minilm_weights/encoder_layer_3_output_LayerNorm_weight.npy\")\n",
    "beta2 = np.load(\"minilm_weights/encoder_layer_3_output_LayerNorm_bias.npy\")\n",
    "\n",
    "ln2_input = ffn_output + ln1_output\n",
    "layer3_output = layer_norm(ln2_input, gamma2, beta2)\n",
    "\n",
    "# Compare with MiniLM's layer 3 output\n",
    "layer3_output_minilm = layer_outputs[\"layer3\"].squeeze(0).cpu().numpy()\n",
    "diff = np.abs(layer3_output - layer3_output_minilm)\n",
    "\n",
    "print(\"✅ Layer 3 Complete\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Close match (1e-6 tolerance):\", np.allclose(layer3_output, layer3_output_minilm, atol=1e-6))\n",
    "print(\"✅ Close match (1e-5 tolerance):\", np.allclose(layer3_output, layer3_output_minilm, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b871062f-faff-4a77-a1d2-b173c3cdb2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer 4 Complete\n",
      "🔍 Max diff: 1.9073486e-06\n",
      "🔍 Mean diff: 1.9057532e-07\n",
      "✅ Close match (1e-6 tolerance): True\n",
      "✅ Close match (1e-5 tolerance): True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Set up a hook to capture the intermediate outputs from MiniLM ---\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output[0].detach().clone()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on MiniLM layers\n",
    "hooks = []\n",
    "hooks.append(model.encoder.layer[3].register_forward_hook(get_layer_output(\"layer3\")))\n",
    "hooks.append(model.encoder.layer[4].register_forward_hook(get_layer_output(\"layer4\")))\n",
    "\n",
    "# Run forward pass to capture outputs\n",
    "with torch.no_grad():\n",
    "    _ = model(input_ids, attention_mask=torch.ones_like(input_ids))\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Get MiniLM's layer 3 output to use as input to our layer 4\n",
    "layer3_output_minilm = layer_outputs[\"layer3\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "# ----- Layer 4 Implementation -----\n",
    "# 1. Self-Attention Block\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_4_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_4_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_4_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_4_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_4_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_4_attention_self_value_bias.npy\")\n",
    "W_o = np.load(\"minilm_weights/encoder_layer_4_attention_output_dense_weight.npy\")\n",
    "b_o = np.load(\"minilm_weights/encoder_layer_4_attention_output_dense_bias.npy\")\n",
    "\n",
    "# Use MiniLM's layer 3 output as input\n",
    "Q_manual = layer3_output_minilm @ W_q.T + b_q\n",
    "K_manual = layer3_output_minilm @ W_k.T + b_k\n",
    "V_manual = layer3_output_minilm @ W_v.T + b_v\n",
    "\n",
    "# Multi-head attention\n",
    "num_heads = 12\n",
    "head_dim = Q_manual.shape[1] // num_heads\n",
    "seq_len = Q_manual.shape[0]\n",
    "\n",
    "# Split into heads\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "V_heads = np.split(V_manual, num_heads, axis=-1)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "head_outputs = []\n",
    "for i in range(num_heads):\n",
    "    Q = Q_heads[i]\n",
    "    K = K_heads[i]\n",
    "    V = V_heads[i]\n",
    "    \n",
    "    scores = Q @ K.T / np.sqrt(head_dim)\n",
    "    weights = softmax(scores)\n",
    "    head_output = weights @ V\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "multihead_output = np.concatenate(head_outputs, axis=-1)\n",
    "attn_output = multihead_output @ W_o.T + b_o\n",
    "\n",
    "# First Residual + LayerNorm\n",
    "gamma1 = np.load(\"minilm_weights/encoder_layer_4_attention_output_LayerNorm_weight.npy\")\n",
    "beta1 = np.load(\"minilm_weights/encoder_layer_4_attention_output_LayerNorm_bias.npy\")\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln1_input = attn_output + layer3_output_minilm\n",
    "ln1_output = layer_norm(ln1_input, gamma1, beta1)\n",
    "\n",
    "# Feed-Forward Network\n",
    "W_intermediate = np.load(\"minilm_weights/encoder_layer_4_intermediate_dense_weight.npy\")\n",
    "b_intermediate = np.load(\"minilm_weights/encoder_layer_4_intermediate_dense_bias.npy\")\n",
    "W_output = np.load(\"minilm_weights/encoder_layer_4_output_dense_weight.npy\")\n",
    "b_output = np.load(\"minilm_weights/encoder_layer_4_output_dense_bias.npy\")\n",
    "\n",
    "# Use PyTorch's GELU for exact match with MiniLM\n",
    "intermediate = ln1_output @ W_intermediate.T + b_intermediate\n",
    "intermediate_tensor = torch.tensor(intermediate).float()\n",
    "intermediate_act = torch.nn.functional.gelu(intermediate_tensor).numpy()\n",
    "ffn_output = intermediate_act @ W_output.T + b_output\n",
    "\n",
    "# Second Residual + LayerNorm\n",
    "gamma2 = np.load(\"minilm_weights/encoder_layer_4_output_LayerNorm_weight.npy\")\n",
    "beta2 = np.load(\"minilm_weights/encoder_layer_4_output_LayerNorm_bias.npy\")\n",
    "\n",
    "ln2_input = ffn_output + ln1_output\n",
    "layer4_output = layer_norm(ln2_input, gamma2, beta2)\n",
    "\n",
    "# Compare with MiniLM's layer 4 output\n",
    "layer4_output_minilm = layer_outputs[\"layer4\"].squeeze(0).cpu().numpy()\n",
    "diff = np.abs(layer4_output - layer4_output_minilm)\n",
    "\n",
    "print(\"✅ Layer 4 Complete\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Close match (1e-6 tolerance):\", np.allclose(layer4_output, layer4_output_minilm, atol=1e-6))\n",
    "print(\"✅ Close match (1e-5 tolerance):\", np.allclose(layer4_output, layer4_output_minilm, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d8a7f83-bb30-44b7-97c3-ce41fb56bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer 5 Complete\n",
      "🔍 Max diff: 1.0728836e-06\n",
      "🔍 Mean diff: 1.2315839e-07\n",
      "✅ Close match (1e-6 tolerance): True\n",
      "✅ Close match (1e-5 tolerance): True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Set up a hook to capture the intermediate outputs from MiniLM ---\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_output(name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[name] = output[0].detach().clone()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on MiniLM layers\n",
    "hooks = []\n",
    "hooks.append(model.encoder.layer[4].register_forward_hook(get_layer_output(\"layer4\")))\n",
    "hooks.append(model.encoder.layer[5].register_forward_hook(get_layer_output(\"layer5\")))\n",
    "\n",
    "# Run forward pass to capture outputs\n",
    "with torch.no_grad():\n",
    "    _ = model(input_ids, attention_mask=torch.ones_like(input_ids))\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Get MiniLM's layer 4 output to use as input to our layer 5\n",
    "layer4_output_minilm = layer_outputs[\"layer4\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "# ----- Layer 5 Implementation -----\n",
    "# 1. Self-Attention Block\n",
    "W_q = np.load(\"minilm_weights/encoder_layer_5_attention_self_query_weight.npy\")\n",
    "b_q = np.load(\"minilm_weights/encoder_layer_5_attention_self_query_bias.npy\")\n",
    "W_k = np.load(\"minilm_weights/encoder_layer_5_attention_self_key_weight.npy\")\n",
    "b_k = np.load(\"minilm_weights/encoder_layer_5_attention_self_key_bias.npy\")\n",
    "W_v = np.load(\"minilm_weights/encoder_layer_5_attention_self_value_weight.npy\")\n",
    "b_v = np.load(\"minilm_weights/encoder_layer_5_attention_self_value_bias.npy\")\n",
    "W_o = np.load(\"minilm_weights/encoder_layer_5_attention_output_dense_weight.npy\")\n",
    "b_o = np.load(\"minilm_weights/encoder_layer_5_attention_output_dense_bias.npy\")\n",
    "\n",
    "# Use MiniLM's layer 4 output as input\n",
    "Q_manual = layer4_output_minilm @ W_q.T + b_q\n",
    "K_manual = layer4_output_minilm @ W_k.T + b_k\n",
    "V_manual = layer4_output_minilm @ W_v.T + b_v\n",
    "\n",
    "# Multi-head attention\n",
    "num_heads = 12\n",
    "head_dim = Q_manual.shape[1] // num_heads\n",
    "seq_len = Q_manual.shape[0]\n",
    "\n",
    "# Split into heads\n",
    "Q_heads = np.split(Q_manual, num_heads, axis=-1)\n",
    "K_heads = np.split(K_manual, num_heads, axis=-1)\n",
    "V_heads = np.split(V_manual, num_heads, axis=-1)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "head_outputs = []\n",
    "for i in range(num_heads):\n",
    "    Q = Q_heads[i]\n",
    "    K = K_heads[i]\n",
    "    V = V_heads[i]\n",
    "    \n",
    "    scores = Q @ K.T / np.sqrt(head_dim)\n",
    "    weights = softmax(scores)\n",
    "    head_output = weights @ V\n",
    "    head_outputs.append(head_output)\n",
    "\n",
    "multihead_output = np.concatenate(head_outputs, axis=-1)\n",
    "attn_output = multihead_output @ W_o.T + b_o\n",
    "\n",
    "# First Residual + LayerNorm\n",
    "gamma1 = np.load(\"minilm_weights/encoder_layer_5_attention_output_LayerNorm_weight.npy\")\n",
    "beta1 = np.load(\"minilm_weights/encoder_layer_5_attention_output_LayerNorm_bias.npy\")\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    norm = (x - mean) / np.sqrt(var + eps)\n",
    "    return norm * gamma + beta\n",
    "\n",
    "ln1_input = attn_output + layer4_output_minilm\n",
    "ln1_output = layer_norm(ln1_input, gamma1, beta1)\n",
    "\n",
    "# Feed-Forward Network\n",
    "W_intermediate = np.load(\"minilm_weights/encoder_layer_5_intermediate_dense_weight.npy\")\n",
    "b_intermediate = np.load(\"minilm_weights/encoder_layer_5_intermediate_dense_bias.npy\")\n",
    "W_output = np.load(\"minilm_weights/encoder_layer_5_output_dense_weight.npy\")\n",
    "b_output = np.load(\"minilm_weights/encoder_layer_5_output_dense_bias.npy\")\n",
    "\n",
    "# Use PyTorch's GELU for exact match with MiniLM\n",
    "intermediate = ln1_output @ W_intermediate.T + b_intermediate\n",
    "intermediate_tensor = torch.tensor(intermediate).float()\n",
    "intermediate_act = torch.nn.functional.gelu(intermediate_tensor).numpy()\n",
    "ffn_output = intermediate_act @ W_output.T + b_output\n",
    "\n",
    "# Second Residual + LayerNorm\n",
    "gamma2 = np.load(\"minilm_weights/encoder_layer_5_output_LayerNorm_weight.npy\")\n",
    "beta2 = np.load(\"minilm_weights/encoder_layer_5_output_LayerNorm_bias.npy\")\n",
    "\n",
    "ln2_input = ffn_output + ln1_output\n",
    "layer5_output = layer_norm(ln2_input, gamma2, beta2)\n",
    "\n",
    "# Compare with MiniLM's layer 5 output\n",
    "layer5_output_minilm = layer_outputs[\"layer5\"].squeeze(0).cpu().numpy()\n",
    "diff = np.abs(layer5_output - layer5_output_minilm)\n",
    "\n",
    "print(\"✅ Layer 5 Complete\")\n",
    "print(\"🔍 Max diff:\", diff.max())\n",
    "print(\"🔍 Mean diff:\", diff.mean())\n",
    "print(\"✅ Close match (1e-6 tolerance):\", np.allclose(layer5_output, layer5_output_minilm, atol=1e-6))\n",
    "print(\"✅ Close match (1e-5 tolerance):\", np.allclose(layer5_output, layer5_output_minilm, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72f49375-4086-4509-bcf0-4a999d8d8bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied pooler transformation\n",
      "🔍 Pooler transformation max diff: 2.9802322387695312e-08\n",
      "🔍 CLS token max diff: 9.19537353515625\n",
      "🔍 Mean pooling max diff: 2.8654418024751873\n",
      "MiniLM uses custom pooler transformation\n",
      "\n",
      "Similarity with MiniLM embedding: 1.0\n",
      "Final embedding shape: (384,)\n",
      "First 5 dimensions: [-0.00913658  0.05209111  0.04688694  0.0240268  -0.10534714]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Get MiniLM pooler weights ---\n",
    "# First, check if there's a pooler layer in the model\n",
    "if hasattr(model, 'pooler'):\n",
    "    pooler_dense_weight = model.pooler.dense.weight.detach().cpu().numpy()\n",
    "    pooler_dense_bias = model.pooler.dense.bias.detach().cpu().numpy()\n",
    "    has_pooler = True\n",
    "else:\n",
    "    has_pooler = False\n",
    "    print(\"MiniLM doesn't have an explicit pooler layer\")\n",
    "\n",
    "# --- Get the final hidden states directly from MiniLM ---\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=torch.ones_like(input_ids)).last_hidden_state.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Get the official pooler output if available\n",
    "    if has_pooler:\n",
    "        pooler_output = model(input_ids, attention_mask=torch.ones_like(input_ids)).pooler_output.squeeze(0).cpu().numpy()\n",
    "    else:\n",
    "        # If no pooler, try to get the sentence embedding through the model's API\n",
    "        try:\n",
    "            pooler_output = model.get_sentence_embedding(input_ids).squeeze(0).cpu().numpy()\n",
    "        except:\n",
    "            pooler_output = None\n",
    "            print(\"Could not get official sentence embedding\")\n",
    "\n",
    "# --- Try different pooling strategies ---\n",
    "# 1. CLS token\n",
    "cls_token = last_hidden_states[0]  # First token is [CLS]\n",
    "\n",
    "# 2. Mean pooling\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    mask_expanded = np.expand_dims(attention_mask, -1)\n",
    "    sum_embeddings = np.sum(token_embeddings * mask_expanded, axis=0)\n",
    "    sum_mask = np.sum(mask_expanded, axis=0)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "attention_mask = np.ones(last_hidden_states.shape[0])\n",
    "mean_pooled = mean_pooling(last_hidden_states, attention_mask)\n",
    "\n",
    "# 3. If there's a pooler, apply it to the CLS token\n",
    "if has_pooler:\n",
    "    # The pooler typically applies a dense layer followed by tanh\n",
    "    pooler_manual = np.tanh(cls_token @ pooler_dense_weight.T + pooler_dense_bias)\n",
    "    print(\"Applied pooler transformation\")\n",
    "\n",
    "# --- Compare with MiniLM's pooler output ---\n",
    "if pooler_output is not None:\n",
    "    cls_diff = np.abs(cls_token - pooler_output).max() if pooler_output is not None else float('inf')\n",
    "    mean_diff = np.abs(mean_pooled - pooler_output).max() if pooler_output is not None else float('inf')\n",
    "    \n",
    "    if has_pooler:\n",
    "        pooler_diff = np.abs(pooler_manual - pooler_output).max()\n",
    "        print(f\"🔍 Pooler transformation max diff: {pooler_diff}\")\n",
    "    \n",
    "    print(f\"🔍 CLS token max diff: {cls_diff}\")\n",
    "    print(f\"🔍 Mean pooling max diff: {mean_diff}\")\n",
    "\n",
    "    # Find the best match\n",
    "    if has_pooler and pooler_diff < min(cls_diff, mean_diff):\n",
    "        print(\"MiniLM uses custom pooler transformation\")\n",
    "        final_embedding = pooler_manual\n",
    "    elif cls_diff < mean_diff:\n",
    "        print(\"MiniLM uses CLS token\")\n",
    "        final_embedding = cls_token\n",
    "    else:\n",
    "        print(\"MiniLM uses mean pooling\")\n",
    "        final_embedding = mean_pooled\n",
    "else:\n",
    "    # If we couldn't get the pooler output, default to mean pooling\n",
    "    print(\"Defaulting to mean pooling (couldn't verify)\")\n",
    "    final_embedding = mean_pooled\n",
    "\n",
    "# --- Normalize and compare ---\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    return v / norm if norm > 0 else v\n",
    "\n",
    "final_normalized = normalize(final_embedding)\n",
    "if pooler_output is not None:\n",
    "    pooler_normalized = normalize(pooler_output)\n",
    "    similarity = np.dot(final_normalized, pooler_normalized)\n",
    "    print(f\"\\nSimilarity with MiniLM embedding: {similarity}\")\n",
    "    print(f\"Final embedding shape: {final_embedding.shape}\")\n",
    "    print(f\"First 5 dimensions: {final_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0850955-bc06-4195-b5c2-f66171e76998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MiniLM Sentence Encoding Reproduction Complete\n",
      "==================================================\n",
      "🔍 Final hidden states max difference: 1.0728836059570312e-06\n",
      "🔍 Pooler output max difference: 7.450580596923828e-08\n",
      "🔍 Hidden states cosine similarity: 1.0000001192092896\n",
      "🔍 Pooler output cosine similarity: 1.0\n",
      "==================================================\n",
      "\n",
      "Final Sentence Embeddings (first 10 dimensions):\n",
      "--------------------------------------------------\n",
      "MiniLM official:   [-0.00913658  0.05209111  0.04688694  0.02402681 -0.10534714 -0.02789352\n",
      "  0.0032652   0.0857238  -0.01132614 -0.00611972]\n",
      "Manual reproduction: [-0.0091366   0.05209111  0.04688695  0.02402678 -0.10534718 -0.0278935\n",
      "  0.0032652   0.08572381 -0.01132614 -0.00611977]\n",
      "--------------------------------------------------\n",
      "\n",
      "Shape of final embedding: (384,)\n",
      "This vector can now be used for sentence similarity, classification, etc.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Get MiniLM pooler weights ---\n",
    "pooler_dense_weight = model.pooler.dense.weight.detach().cpu().numpy()\n",
    "pooler_dense_bias = model.pooler.dense.bias.detach().cpu().numpy()\n",
    "\n",
    "# --- Get official MiniLM outputs for comparison ---\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=torch.ones_like(input_ids))\n",
    "    # Last hidden state and pooler output\n",
    "    minilm_last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "    minilm_pooler_output = outputs.pooler_output.squeeze(0).cpu().numpy()\n",
    "\n",
    "# --- Manual implementation ---\n",
    "# 1. Use our layer 5 output as the final hidden states\n",
    "final_hidden_states = layer5_output\n",
    "\n",
    "# 2. Apply the pooler transformation to the CLS token (first token)\n",
    "cls_token = final_hidden_states[0]  # Extract CLS token representation\n",
    "pooler_output_manual = np.tanh(cls_token @ pooler_dense_weight.T + pooler_dense_bias)\n",
    "\n",
    "# --- Compare the results ---\n",
    "# Difference between our manual hidden states and MiniLM's\n",
    "hidden_diff = np.abs(final_hidden_states - minilm_last_hidden).max()\n",
    "# Difference between our manual pooler output and MiniLM's\n",
    "pooler_diff = np.abs(pooler_output_manual - minilm_pooler_output).max()\n",
    "\n",
    "# Compute cosine similarity to show vector alignment\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "hidden_sim = cosine_similarity(final_hidden_states.flatten(), minilm_last_hidden.flatten())\n",
    "pooler_sim = cosine_similarity(pooler_output_manual, minilm_pooler_output)\n",
    "\n",
    "# Display results\n",
    "print(\"✅ MiniLM Sentence Encoding Reproduction Complete\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"🔍 Final hidden states max difference: {hidden_diff}\")\n",
    "print(f\"🔍 Pooler output max difference: {pooler_diff}\")\n",
    "print(f\"🔍 Hidden states cosine similarity: {hidden_sim}\")\n",
    "print(f\"🔍 Pooler output cosine similarity: {pooler_sim}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare the first 10 dimensions of both embeddings\n",
    "print(\"\\nFinal Sentence Embeddings (first 10 dimensions):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"MiniLM official:  \", minilm_pooler_output[:10])\n",
    "print(\"Manual reproduction:\", pooler_output_manual[:10])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nShape of final embedding:\", pooler_output_manual.shape)\n",
    "print(\"This vector can now be used for sentence similarity, classification, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c7a25-c775-4bd8-92d4-00952000c242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
